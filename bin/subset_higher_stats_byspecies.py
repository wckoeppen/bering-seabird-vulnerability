# -*- coding: utf-8 -*-
# <nbformat>3.0</nbformat>

# <markdowncell>

# # Subset higher order stats by our bounding boxes

# <codecell>

#A list of imports we need for code later in the notebook.
#The css_styles() function must go last.
%matplotlib inline
from owslib.wfs import WebFeatureService
import json
from utilities import find_dict_keys
from shapely.geometry import shape, MultiPolygon
from shapely.geometry import box

import folium
from utilities import get_coords
from IPython.core.display import HTML

import time
import numpy as np
from numpy import ma
import netCDF4
import pandas as pd
from pandas import Series

import matplotlib as mpl
from matplotlib import cm
import matplotlib.pyplot as plt
from matplotlib import ticker

from utilities import css_styles
css_styles()

# <codecell>

#Load our important bird areas again.
known_wfs = "http://solo.axiomalaska.com/geoserver/audubon_ibav3/ows"
wfs = WebFeatureService(known_wfs, version='1.0.0')
geojson_response = wfs.getfeature(typename=['audubon_ibav3:audubon_ibas_v3_single_spp_core_areas_20aug2014'],
                                  outputFormat="application/json",
                                  srsname="urn:x-ogc:def:crs:EPSG:4326").read()
geojson = json.loads(geojson_response)

geometries = find_dict_keys('geometry', geojson)
shapes = [shape(g) for g in geometries]

sitenames = find_dict_keys('sitename', geojson)
sitenames = [str(s) for s in sitenames]
nsites=len(sitenames)

iba_types = find_dict_keys('iba_type', geojson)
iba_types = [str(s) for s in iba_types]

profiles = find_dict_keys('profile', geojson)
profiles = [str(s) for s in profiles]

species = find_dict_keys('species', geojson)
species = [str(s) for s in species]

#This generates bounding boxes from the complex geometries in shapes.
minlat = list()
minlon = list()
maxlat = list()
maxlon = list()
for s in shapes:
    minlat.append(s.bounds[0])
    minlon.append(s.bounds[1])
    maxlat.append(s.bounds[2])
    maxlon.append(s.bounds[3])
# miny, minx, maxy, maxx

# <codecell>

#Longitudes in the model are in positive east.
#Longitudes in the shape files are -180/180, which I'll convert to the model coordinates

for x in range(nsites):
    if minlon[x] < 0:
        minlon[x] = minlon[x]+360.
    if maxlon[x] < 0:
        maxlon[x] = maxlon[x]+360.

# <codecell>

#Hindcast
#These are the H1/H2 stats of the CCCma model
#NOTE: in the respository, these files are gzipped and must be gunzipped before this cell will work.
avg =     netCDF4.Dataset('/home/will/Projects/bering-seabird-vulnerability/resources/cccma_h1_average.nc')
stddev =  netCDF4.Dataset('/home/will/Projects/bering-seabird-vulnerability/resources/cccma_h1_rmssdn.nc')
pavg =    netCDF4.Dataset('/home/will/Projects/bering-seabird-vulnerability/resources/cccma_h2_average.nc')
pstddev = netCDF4.Dataset('/home/will/Projects/bering-seabird-vulnerability/resources/cccma_h2_rmssdn.nc')

#These were the previous stats
#avg = netCDF4.Dataset('~/Projects/bering-seabird-vulnerability/Resources/core_average.nc')
#stddev = netCDF4.Dataset('~/Projects/bering-seabird-vulnerability/Resources/core_rmssdn.nc')
#pavg = netCDF4.Dataset('~/Projects/bering-seabird-vulnerability/Resources/cccma_average.nc')
#pstddev = netCDF4.Dataset('~/Projects/bering-seabird-vulnerability/Resources/cccma_rmssdn.nc')

# <codecell>

# Load latitude and longitude arrays
latitude = np.array(avg.variables['LATITUDE'])
longitude = np.array(avg.variables['LONGITUDE'])

# <codecell>

# Set up depth bins, for these data, they are in meters

depthbins = pd.DataFrame({'mindepth': [0, 75],
                          'maxdepth': [60, 200]},
                         index=['shallow','deep'],
                         columns=['mindepth', 'maxdepth', 'minz', 'maxz'])

depth = np.array(avg.variables['zsalt'])
depthindices=list()
for d in range(len(depthbins.index)):
    indicesz=np.where(np.logical_and(depth[:] <= depthbins.maxdepth[d],
                                     depth[:] >= depthbins.mindepth[d]))
    depthindices.append(indicesz[0])
    if len(depthindices[d]) > 0:
        depthbins.minz[d] = min(depthindices[d])
        depthbins.maxz[d] = max(depthindices[d])
        
depthbins['label']=['0 to 60 m', '75 to 200 m']
        
depthbins

# <markdowncell>

# But each of these also need four slots for the [avg, stddev, pavg, pstddev]. I think it's best to create a new dataframe for each variable, test for depth, and if it has depth create the bins and add the columns to the individual dataframe. At the end, we can add all the dataframes together for output via a CSV.

# <codecell>

#This is our known metadata for the outputs
metadata = pd.DataFrame({
         'orgName': [
                   #these don't have depth
                   'icephl_latlon','ben_latlon',
                   'aice_latlon',
                   #these do
                 'phs_latlon','phl_latlon','mzl_latlon','cop_latlon','ncao_latlon','ncas_latlon','eup_latlon','det_latlon',
                 'temp_latlon',
                 'u_latlon',
                 'v_latlon',],
         'name': ['Ice Phytoplankton Concentration',
                  'Benthos Concentration',
                  'Sea Ice Area Fraction',
                  'Small Phytoplankton Concentration',
                  'Large Phytoplankton Concentration',
                  'Large Microzooplankton Concentration',
                  'Small Coastal Copepod Concentration',
                  'Offshore Neocalanus Concentration',
                  'Neocalanus Concentration',
                  'Euphausiids Concentration',
                  'Detritus Concentration',
                  'Sea Water Temperature',
                  'Zonal (U) Current',
                  'Meridional (V) Current'],
         'units': ['mgC/m2','mgC/m2',
                   'Fraction',
                   'mgC/m3','mgC/m3','mgC/m3','mgC/m3','mgC/m3','mgC/m3','mgC/m3','mgC/m3',
                   'degrees C',
                   'm/s',
                   'm/s']
       })

# <markdowncell>

# Some shapes will be empty (i.e., there's a polygon, but no model data), others will have very few values. We should keep track of how many model pixels are going into each resulting mean. For the combined IBAs, there are 210 polygons, identified by sitenums, objectids, and their geometries.

# <markdowncell>

# ##Store the results by keeping track of the data in a Panda Dataframes
# We will append columns to these and fill in the data as we get it.

# <codecell>

#Create a dataframe to house the IBA site information, indexed by the IBA sitenums.
siteinfo = pd.DataFrame(
                        {
                        'sitename': sitenames,
                        'iba_type': iba_types,
                        'profile': profiles,
                        'species': species,
                        'minlat': minlat,
                        'minlon': minlon,
                        'maxlat': maxlat,
                        'maxlon': maxlon,
                        'tested': (['no'] * nsites)
                        }
                       )

#We'll also add one more row, for the whole area, as a reference
areainfo = pd.DataFrame(
                        {
                        'minlat': min(latitude),
                        'minlon': min(longitude),
                        'maxlat': max(latitude),
                        'maxlon': max(longitude),
                        'miny': [0],
                        'minx': [0],
                        'maxy': [len(latitude)],
                        'maxx': [len(longitude)],
                        'tested': ['yes']
                        },
                        index = ['Whole Area']
                       )

siteinfo=siteinfo.append(areainfo)
siteinfo=siteinfo[['sitename', 'iba_type', 'profile','species', 'minlat', 'maxlat', 'minlon', 'maxlon', 'miny', 'maxy', 'minx', 'maxx', 'tested']]
nsites=nsites+1

# <markdowncell>

# Now convert the latlons to pixel indices for the model.

# <codecell>

latindices = list()
lonindices = list()

for x in range(nsites):
    #first do latitude
    indicesy = np.where(np.logical_and(latitude <= siteinfo.maxlat[x],
                                       latitude >= siteinfo.minlat[x]))
    latindices.append(indicesy[0])
    #then longitude
    #PMEL models are in positive east, so we have to convert our bounding box.
    indicesx = np.where(np.logical_and(longitude[:] <= siteinfo.maxlon[x],
                                       longitude[:] >= siteinfo.minlon[x]))
    lonindices.append(indicesx[0])

# <codecell>

#finds the minimum and maxium pixel indices for each sitenum
avgdata = ma.masked_array(avg.variables[metadata.orgName[0]][:])
for x in range(nsites-1):
    if len(latindices[x]) > 0:
        siteinfo.miny[x] = min(latindices[x])
        siteinfo.maxy[x] = max(latindices[x])
    if len(lonindices[x]) > 0:
        siteinfo.minx[x] = min(lonindices[x])
        siteinfo.maxx[x] = max(lonindices[x]) 
#This tests to see if there are any NaNs, meaning the IBA goes off the edge of the model.
#We'll skip any of those IBAs by setting a "Tested" variable to 0.
    if pd.notnull(siteinfo.miny[x]):
        if pd.notnull(siteinfo.maxy[x]):
            if pd.notnull(siteinfo.minx[x]):
                if pd.notnull(siteinfo.maxx[x]):
                    #This will find if there are any non-masked values to test.
                    count = ma.count(avgdata[0, siteinfo.miny[x]:siteinfo.maxy[x]+1, siteinfo.minx[x]:siteinfo.maxx[x]+1])
                    if count > 0:
                        siteinfo.tested[x] = 'yes'

# <markdowncell>

# ##Process the data into a list of dataframes, indexed by sitenum

# <markdowncell>

# Now that we have the pixel mins and maxes, we can iterate through the variables:
# 
# 1. Check for depth: if present iterate three times for depth bins, if absent, just do once.  
# 2. Using the pixel indexes to subset each model variable for avg, stddev, pavg, and pstddev arrays and average those values.  
# 3. Create a data frame with the mean avg, stddev, pavg, and pstddev for each sitenum, and append to list of dataframe results
# 4. Create a label including the variable name and depth bin, append to another list of results. There should be one label for each dataframe.
# 5. We'll concatenate these dataframes to the siteinfo dataframe into large dataframe for output to CSV.

# <codecell>

nvariables = len(metadata.orgName)
#create an empty dataframe
dfresults = pd.DataFrame(index = siteinfo.index)

#for each variable
for i in range(nvariables): 
    #Load in the data
    avgdata = ma.masked_array(avg.variables[metadata.orgName[i]][:])
    mask=ma.getmask(avgdata)
    stddevdata = ma.masked_array(stddev.variables[metadata.orgName[i]][:], mask)
    pavgdata = ma.masked_array(pavg.variables[metadata.orgName[i]][:], mask)
    pstddevdata = ma.masked_array(pstddev.variables[metadata.orgName[i]][:], mask)
    
    #check for depth
    if len(avgdata.shape) == 3:
        avgvals = list()
        stddevvals = list()
        pavgvals = list()
        pstddevvals = list()
        count = list()
        
        for x in range(nsites):
            if (siteinfo.tested[x] == 'yes'):
                #mean of data(latitude_indices, longitude_indices)
                #The +1 is necessary because numpy indexing goes from start:end, but end is the first value that's NOT included.
                #Whereas the way I've set this up, the maxy and maxx SHOULD be included.
                avgvals.append(ma.mean(avgdata[0, siteinfo.miny[x]:siteinfo.maxy[x]+1, siteinfo.minx[x]:siteinfo.maxx[x]+1]))
                stddevvals.append(ma.mean(stddevdata[0, siteinfo.miny[x]:siteinfo.maxy[x]+1, siteinfo.minx[x]:siteinfo.maxx[x]+1]))
                pavgvals.append(ma.mean(pavgdata[0, siteinfo.miny[x]:siteinfo.maxy[x]+1, siteinfo.minx[x]:siteinfo.maxx[x]+1]))
                pstddevvals.append(ma.mean(pstddevdata[0, siteinfo.miny[x]:siteinfo.maxy[x]+1, siteinfo.minx[x]:siteinfo.maxx[x]+1]))
                count.append(ma.count(avgdata[0, siteinfo.miny[x]:siteinfo.maxy[x]+1, siteinfo.minx[x]:siteinfo.maxx[x]+1]))

            else:
                avgvals.append(np.nan)
                stddevvals.append(np.nan)
                pavgvals.append(np.nan)
                pstddevvals.append(np.nan)
                count.append(np.nan)
        
        dfresults[metadata.name[i]+' Count'] = count
        dfresults[metadata.name[i]+' Avg'] = avgvals
        dfresults[metadata.name[i]+' StdDev'] = stddevvals
        dfresults[metadata.name[i]+' PAvg'] = pavgvals
        dfresults[metadata.name[i]+' PStdDEv'] = pstddevvals


    if len(avgdata.shape) == 4:
        for d in range(len(depthbins.index)):
            avgvals = list()
            stddevvals = list()
            pavgvals = list()
            pstddevvals = list()
            count = list()
            
            for x in range(nsites):
                
                if (siteinfo.tested[x] == 'yes'):
                    #mean of data(latitude_indices, longitude_indices)
                    #The +1 is necessary because numpy indexing goes from start:end, but end is the first value that's NOT included.
                    #Whereas the way I've set this up, the maxy and maxx SHOULD be included.
                    count.append(ma.count(avgdata[0, depthbins.minz[d]:depthbins.maxz[d]+1,
                              siteinfo.miny[x]:siteinfo.maxy[x]+1,
                              siteinfo.minx[x]:siteinfo.maxx[x]+1]))
                    if (count[x] > 0):
                        avgvals.append(ma.mean(avgdata[0, depthbins.minz[d]:depthbins.maxz[d]+1,
                                                       siteinfo.miny[x]:siteinfo.maxy[x]+1,
                                                       siteinfo.minx[x]:siteinfo.maxx[x]+1]))
                        stddevvals.append(ma.mean(stddevdata[0, depthbins.minz[d]:depthbins.maxz[d]+1,
                                                             siteinfo.miny[x]:siteinfo.maxy[x]+1,
                                                             siteinfo.minx[x]:siteinfo.maxx[x]+1]))
                        pavgvals.append(ma.mean(pavgdata[0, depthbins.minz[d]:depthbins.maxz[d]+1,
                                                         siteinfo.miny[x]:siteinfo.maxy[x]+1,
                                                         siteinfo.minx[x]:siteinfo.maxx[x]+1]))
                        pstddevvals.append(ma.mean(pstddevdata[0, depthbins.minz[d]:depthbins.maxz[d]+1,
                                                               siteinfo.miny[x]:siteinfo.maxy[x]+1,
                                                               siteinfo.minx[x]:siteinfo.maxx[x]+1]))
                    else:
                        avgvals.append(np.nan)
                        stddevvals.append(np.nan)
                        pavgvals.append(np.nan)
                        pstddevvals.append(np.nan)
                else:
                    avgvals.append(np.nan)
                    stddevvals.append(np.nan)
                    pavgvals.append(np.nan)
                    pstddevvals.append(np.nan)
                    count.append(np.nan)
                    
            dfresults[metadata.name[i]+' '+depthbins.label[d]+' Count'] = count
            dfresults[metadata.name[i]+' '+depthbins.label[d]+' Avg' + ' in ' + metadata.units[i]] = avgvals
            dfresults[metadata.name[i]+' '+depthbins.label[d]+' StdDev' + ' in ' + metadata.units[i]] = stddevvals
            dfresults[metadata.name[i]+' '+depthbins.label[d]+' PAvg' + ' in ' + metadata.units[i]] = pavgvals
            dfresults[metadata.name[i]+' '+depthbins.label[d]+' PStdDEv' + ' in ' + metadata.units[i]] = pstddevvals

# <markdowncell>

# ## Export to CSV
# Combine the siteinfo and dfresults dataframes, and write out to CSV.

# <codecell>

merged = siteinfo.join(dfresults)
merged.to_csv('/home/will/Projects/bering-seabird-vulnerability/outputs/cccma_h1h2_coreareas.csv',index_label='localindex')

# <codecell>


