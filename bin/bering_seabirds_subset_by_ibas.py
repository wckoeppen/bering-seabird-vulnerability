
# coding: utf-8

# # Bering Seabirds and Future Climate Variability
# Assessing the projected change in climate variability of areas used by seabirds in the Bering Sea. This notebook is a self-contained (and well documented) code base to subset climate projections stored as NetCDF files by Important Bird Area polygons (hosted by the Alaska Ocean Observing System).

# In[154]:

#A list of imports we need for code later in the notebook.
#The css_styles() function must go last.
get_ipython().magic(u'matplotlib inline')
from owslib.wfs import WebFeatureService
import json
from utilities import find_dict_keys
import fiona
from shapely.geometry import shape, MultiPolygon
from shapely.geometry import box

import folium
from cartopy.io import shapereader
from utilities import get_coords
from IPython.core.display import HTML

import time
import numpy as np
from numpy import ma
import netCDF4
import pandas as pd
from pandas import Series

import matplotlib as mpl
from matplotlib import cm
import matplotlib.pyplot as plt
from matplotlib import ticker

from utilities import css_styles
css_styles()


# ## Load Important Bird Areas
# These are defined by [Audobon](http://web4.audubon.org/bird/iba/) and hosted by AOOS on a geoserver. Currently, they represent areas important for numerous species of birds.
# 
# The single species core areas must be processed differently, because they have different metadata and include species information. They are contained in a separate notebook: [located here](http://127.0.0.1:8888/cc62f4c2-b95f-41a4-9b84-13c46ce9c37a).

# In[155]:

#Load our important bird areas again.
known_wfs = "http://solo.axiomalaska.com/geoserver/audubon_ibav3/ows"
wfs = WebFeatureService(known_wfs, version='1.0.0')
geojson_response = wfs.getfeature(typename=['audubon_ibav3:audubon_ibas_v3_20aug2014'], outputFormat="application/json", srsname="urn:x-ogc:def:crs:EPSG:4326").read()
geojson = json.loads(geojson_response)


# In[156]:

geometries = find_dict_keys('geometry', geojson)
shapes = [shape(g) for g in geometries]

ids = find_dict_keys('id', geojson)
ids=[(str(j).split('.', 1))[1]  for j in ids]

sitenames = find_dict_keys('newsitenam', geojson)
sitenames = [str(s) for s in sitenames]

profiles = find_dict_keys('profile', geojson)
profiles = [str(s) for s in profiles]

nsites=len(sitenames)

#This generates bounding boxes from the complex geometries in shapes.
minlat = list()
minlon = list()
maxlat = list()
maxlon = list()
for s in shapes:
    minlat.append(s.bounds[0])
    minlon.append(s.bounds[1])
    maxlat.append(s.bounds[2])
    maxlon.append(s.bounds[3])
# miny, minx, maxy, maxx


# In[157]:

# lme_shp=shapereader.Reader("/home/will/Projects/bering-seabird-vulnerability/resources/MEOW_LME_Poly_GCS.shp")
# lme_shps = lme_shp.geometries()
# lme_shapes = [shape(l) for l in lme_shps]

# print lme_shps[0]
# for x in range(len(lme_shapes)):
#     print lme_shapes[x]

# nsites = nsites + len(lme_shp)

# #In the interest of time, we're going to jury-rig these
# for s in lme_shapes:
#     minlat.append(s.bounds[1])
#     minlon.append(s.bounds[0])
#     maxlat.append(s.bounds[3])
#     maxlon.append(s.bounds[2])
    
#     print s.bounds[0], s.bounds[1], s.bounds[2], s.bounds[3]


# In[158]:

#In the interest of time, we're going to jury-rig these.
#The problem is that the shape files we were sent have a break at -180/180,
#which is making everything freak out.

# Aleutian Islands
minlat.append(50.7)
minlon.append(171.4)
maxlat.append(53.5)
maxlon.append(-168.8)
ids.append('None')
profiles.append('None')
sitenames.append('Aleutian Island LME')

# Bering Sea
minlat.append(52.0)
minlon.append(172.7)
maxlat.append(61.4)
maxlon.append(-160.1)
ids.append('None')
profiles.append('None')
sitenames.append('West Bering Sea LME')


# Chukchi Sea
minlat.append(61.5)
minlon.append(173.4)
maxlat.append(79.0)
maxlon.append(-155.6)
ids.append('None')
profiles.append('None')
sitenames.append('Chukchi Sea LME')

#Gulf of Alaska
minlat.append(51.0)
minlon.append(-164.0)
maxlat.append(61.0)
maxlon.append(-136.0)
ids.append('None')
profiles.append('None')
sitenames.append('Gulf of Alaska LME')

nsites = nsites + 4


# In[159]:

#Longitudes in the model are in positive east.
#Longitudes in the shape files are -180/180, which I'll convert to the model coordinates

for x in range(len(minlat)):
    if minlon[x] < 0:
        minlon[x] = minlon[x]+360.
    if maxlon[x] < 0:
        maxlon[x] = maxlon[x]+360.
        
#When I do this, some of the shapes that cross the dateline can seem to switch
#We need to switch these back.
for x in range(nsites):
    if minlon[x] > maxlon[x]:
        bigger = minlon[x]
        smaller = maxlon[x]
        maxlon[x]=bigger
        minlon[x]=smaller


# For now, this just uses a bounding box for each IBA rather than polygons. Because actual polygons are hard. Note: don't worry about the wrapped boxes in the display. Those are fixed in reality (just above), but we're plotting from unfixed data.

# In[160]:

mapper = folium.Map(location=[65.1, 200], zoom_start=3)
#Map polygons in red
# for s in shapes:
#     for c in get_coords(s):
#         mapper.line(c, line_color='#FF0000', line_weight=2)

#Map lmes in green        
# for l in lme_shapes:
#     analysis_box = l.bounds
#     bound = box(*analysis_box).boundary.coords
#     mapper.line(bound, line_color='#00FF00', line_weight=3)
    
for s in range(len(minlat)):
    analysis_box = [minlat[s], minlon[s], maxlat[s], maxlon[s]]
    bound = box(*analysis_box).boundary.coords
    mapper.line(bound, line_color='#00FF00', line_weight=3)
    
mapper.lat_lng_popover()
mapper._build_map()
HTML('<iframe srcdoc="{srcdoc}" style="width:100%; height: 535px; border: none"></iframe>'.format(srcdoc=mapper.HTML.replace('"', '&quot;')))


# ## Process model data
# ### Set Climate basline model from which to pull data
# Initially we were going to use the [PMEL Co-ordinated Ocean-Ice Reference Experiments (CORE) Climate Model](http://portal.aoos.org/?v=rand&portal_id=25#module-metadata/5626a0b6-7d79-11e3-ac17-00219bfe5678/82fde9d7-ccdf-4404-a9bd-c159d9d6461d) - 01/19/1969 - 01/02/2005  
# as our baseline time-period. In theory, we have two other models which have data over our baseline period, [CFS-R](http://portal.aoos.org/?v=rand&portal_id=25#module-metadata/f8cb79f6-7d59-11e3-a6ee-00219bfe5678/af7b2758-e7d0-4d82-96eb-1696c464586d) and [CFSv2](http://portal.aoos.org/?v=rand&portal_id=25#module-metadata/4e77fed6-7d7a-11e3-97d7-00219bfe5678/0e50760b-8795-4aac-a155-dd624aff9c24), but we didn't recieve the full time-series for either of these models.
# 
# However, then we realized there is model bias that we can't account for because we have no model data that overlaps with the CORE time frame.
# 
# ### Climate projections(s) from which to pull data
# [PMEL CCCma](http://portal.aoos.org/alaska-statewide.php#module-metadata/4f706756-7d57-11e3-bce5-00219bfe5678/6b2d1f96-4c5b-4b13-ad57-c8ca2e326a34) - 01/26/2003 - 01/01/2040  
# [PMEL MIROC](http://portal.aoos.org/alaska-statewide.php#module-metadata/68ea728a-7d7a-11e3-823b-00219bfe5678/50242dc9-b639-42c4-8287-d0aeb082fb1f) - 12/29/2002 - 12/04/2039  
# [PMEL ECHO-G](http://portal.aoos.org/alaska-statewide.php#module-metadata/18ffa59c-7d7a-11e3-82a4-00219bfe5678/7bfa82c2-3051-484e-8c7b-9b9f711083f5) - 12/29/2002 - 12/04/2039  
# <div class="warning"><strong>CCCma Model Dates</strong> - CCCma models have slightly different dates. This is because the modeling groups started at slightly different times. Nick Bond says these dates should not be rounded (in order to make them all match up) as it will increase model error.</div>
# 
# <div class="warning"><strong>ECHO-G Model Dates</strong> - ECHO-G models cover the same overall time period as CCCma and MIROC, but there are missing 54 weeks interspersed in the dataset. Many of those weeks are from wintertime periods (Nov-Feb). These are not missing files (filenames are consecutively numbered, and all are in order and accounted for), but rather just runs that are not present in the dataset. For this work, we decided to exclude years that are missing more than three weeks per year, which would include the following: 2008, 2014, 2017, 2018, 2019, 2025, 2032, 2039. This still leaves us with 29 out of 37 years total modeled. In the split-year (intra-model comparison), we have 14 (nearly complete) years of models in the first half of the modeled period, and 15 (nearly complete) years of models for the second half.</div>
# 
# Each forecast model produced one time slice per week from 2002 to 2040. Loading these models remotely worked, but they are too large to do analyses over the web. I downloaded these files to my local machine via THREDDS (~294 Gb of files for each model). Unfortunately, by doing the work locally, we lost the advantage of using the ncml virtual aggregated datasets (which makes the whole lot of files looks like a single dataset).Â We also cannot use any of the metadata improvements that AOOS makes using ncml.
# 
# ### Actual Analysis
# We are doing two types of analysis. In one, we compare the projected climate (CCCma, MIROC, or ECHO-G) to a hindcast (CORE). In the other, we compare the second half of a projected model to the first half so we can be sure we are not seeing effects due to model bias. Because the three projected models cover different dates, we can only use dates (model weeks) present in all three projections. The overlapping dates are January 26, 2003 to December 4, 2039. These were split in the second (intra-model) comparison to accomodate the first half (January 26, 2003 - July 11, 2021) and second half (July 18, 2021 - December 4, 2039) of the total period. In model weeks, these translate to:
# 
# - CORE - {1001-2874}
# 
# - CCCma - {1001-2924}
# - CCCma first half - {1001..1964}
# - CCCma second half - {1965..2924}
# 
# - MIROC - {1005..2928}
# - MIROC first half - {1005..1968}
# - MIROC second half - {1969..2928}
# 
# - ECHOG - {1005..2874}
# - ECHOG first half - {1005..1935}
# - ECHOG second half - {1936..2874}
# - NOTE: we've dropped out some of the ECHOG model weeks. See warning above for details.
# 
# ### Processing using NCO
# The model results are quite large, and we have to think a lot about memory management when trying to process the files and calculate statistics. I tried doing it in python, but calculated that the processing would take over 2 months of CPU time. Whereas it takes a few hours using an external package of binaries.
# 
# This uses a series of binary functions that are part of the [NetCDF Operator (NCO)](http://nco.sourceforge.net/) suite. NCO programs are standalone programs that must be installed and run separately. They are documented here for completeness.
# 
# We are trying to calculate first the mean:
# <pre><code>
# FORM
# ncea input_files output_file
# -y ttl: calculates the total of all values
# -y sqravg: calculates the average of all the squares
# 
# </code></pre>
# Once we have the average, we can calculate the differences from the mean using NCBO. This takes the first file, subtracts the second file, and writes to the third file. In our case, we are creating a series of files that represent the difference from the mean.
# <pre><code>
# FORM
# ncbo input_file1 input_file2 output_file
# </code></pre>
# NCBO can only work on one file at a time (it doesn't accept a list of files), so I wrote a bash script that loops through and does an NCBO command for every file in the listed directory.
# 
# NCEA and NCBO are smart enough to use and preserve the variable names in each NetCDF file, so the resulting file will have a processed cube of data for all the variables already. It even preserves the Z column (so only averaging through time, not space). Once we have directory full of anomalies (differences), we NCEA again to calculate the root mean square of the differences (i.e., the standard deviation).
# 
# <pre><code>
# FORM
# ncea -y rmssdn input_files outputfile
# -y rmssdn: calculates the root mean square of the differences.
# </code></pre>
# 
# #### First Half
# ##### NCO
# <pre><code>
# ncea /augie/gluster/data/netCDF/pmel/cccma/cccma_week{1001..1964}.nc /augie/gluster/data/netCDF/pmel/cccma/Outputs/cccma_average_h1.nc
# </code></pre>
# 
# ##### Bash script, made executable and run locally
# <pre><code>
# #!/bin/bash
# count=1001
# while [ $count -le 1964 ]
# do
#     infile="/augie/gluster/data/netCDF/pmel/cccma/cccma_week${count}.nc"
#     mean="/augie/gluster/data/netCDF/pmel/cccma/Outputs/cccma_average_h1.nc"
#     outfile="/augie/gluster/data/netCDF/pmel/cccma/Diff/cccma_diffh1_week${count}.nc"
#     if [ -f $infile ]; #this part skips files if they don't exist
# 	then
# 	    echo "Executing: ncbo $infile $mean $outfile"
# 	    ncbo $infile $mean $outfile
# 	else
# 	    echo "Skipping: $infile"
# 	fi
# 	let count=count+1
# done
# </code></pre>
# 
# ##### NCO
# <pre><code>
# ncea -y rmssdn /augie/gluster/data/netCDF/pmel/cccma/Diff/cccma_diffh1_week*.nc /augie/gluster/data/netCDF/pmel/cccma/Outputs/cccma_h1_rmssdn.nc
# </code></pre>
# 
# #### Second Half
# ##### NCO
# <pre><code>
# ncea /augie/gluster/data/netCDF/pmel/cccma/cccma_week{1965..2928}.nc /augie/gluster/data/netCDF/pmel/cccma/Outputs/cccma_average_h2.nc
# </code></pre>
# 
# ##### Bash script, made executable and run locally
# <pre><code>
# #!/bin/bash
# count=1965
# while [ $count -le 2928 ]
# do
#     infile="/augie/gluster/data/netCDF/pmel/cccma/cccma_week${count}.nc"
#     mean="/augie/gluster/data/netCDF/pmel/cccma/Outputs/cccma_average_h2.nc"
#     outfile="/augie/gluster/data/netCDF/pmel/cccma/Diff/cccma_diffh2_week${count}.nc"
#     if [ -f $infile ];
# 	then
# 	    echo "Executing: ncbo $infile $mean $outfile"
# 	    ncbo $infile $mean $outfile
# 	else
# 	    echo "Skipping: $infile"
# 	fi
# 	let count=count+1
# done
# </code></pre>
# 
# ##### NCO
# <pre><code>
# ncea -y rmssdn /augie/gluster/data/netCDF/pmel/cccma/Diff/cccma_diffh2_week*.nc /augie/gluster/data/netCDF/pmel/cccma/Outputs/cccma_h2_rmssdn.nc
# </code></pre>

# ## Load Processed Model Results
# These statistics outputs are loaded in, so we can do our statistics. Because we're accessing raw NetCDF files instead of aggregated and curated NcML, we need to access the data using the original variable names. These can be found using ncdump, or some other netCDF software. I'm loading these into a panda DataFrame.

# In[161]:

#The original metadata was not very good, and not all fields were filled in (e.g., units),
#so we have to correct these. This is our known metadata for the outputs:
metadata = pd.DataFrame({
         'orgName': [
                   #these don't have depth
                   'icephl_latlon','ben_latlon',
                   'aice_latlon',
                   #these do
                 'phs_latlon','phl_latlon','mzl_latlon','cop_latlon','ncao_latlon','ncas_latlon','eup_latlon','det_latlon',
                 'temp_latlon',
                 'u_latlon',
                 'v_latlon',],
         'name': ['Ice Phytoplankton Concentration',
                  'Benthos Concentration',
                  'Sea Ice Area Fraction',
                  'Small Phytoplankton Concentration',
                  'Large Phytoplankton Concentration',
                  'Large Microzooplankton Concentration',
                  'Small Coastal Copepod Concentration',
                  'Offshore Neocalanus Concentration',
                  'Neocalanus Concentration',
                  'Euphausiids Concentration',
                  'Detritus Concentration',
                  'Sea Water Temperature',
                  'Zonal (U) Current',
                  'Meridional (V) Current'],
         'units': ['mgC/m2','mgC/m2',
                   'Fraction',
                   'mgC/m3','mgC/m3','mgC/m3','mgC/m3','mgC/m3','mgC/m3','mgC/m3','mgC/m3',
                   'degrees C',
                   'm/s',
                   'm/s']
       })


# In[162]:

output_directory = '/home/will/Projects/bering-seabird-vulnerability/outputs/'
resource_directory = '/home/will/Projects/bering-seabird-vulnerability/resources/'

#(1) in the respository, these files are gzipped and must be gunzipped before this cell will work,
#(2) change the filenames to match your system
#(3) you need the full path, ipython doesn't seem to understand ~/.

# CCCma vs Core
output_filename = 'cccma_core_IBAs.csv'
figure_basename = 'cccma_core'
avg =     netCDF4.Dataset(resource_directory + 'core_average.nc')
stddev =  netCDF4.Dataset(resource_directory + 'core_rmssdn.nc')
pavg =    netCDF4.Dataset(resource_directory + 'cccma_average.nc')
pstddev = netCDF4.Dataset(resource_directory + 'cccma_rmssdn.nc')

# #First half vs second half of CCCma
# output_filename = 'cccma_h1h2_IBAs.csv'
# figure_basename = 'cccma_h1h2'
# avg =     netCDF4.Dataset(resource_directory + 'cccma_h1_average.nc')
# stddev =  netCDF4.Dataset(resource_directory + 'cccma_h1_rmssdn.nc')
# pavg =    netCDF4.Dataset(resource_directory + 'cccma_h2_average.nc')
# pstddev = netCDF4.Dataset(resource_directory + 'cccma_h2_rmssdn.nc')

# #MIROC vs CORE
# output_filename = 'miroc_core_IBAs.csv'
# figure_basename = 'miroc_core'
# avg =     netCDF4.Dataset(resource_directory + 'core_average.nc')
# stddev =  netCDF4.Dataset(resource_directory + 'core_rmssdn.nc')
# pavg =    netCDF4.Dataset(resource_directory + 'miroc_average.nc')
# pstddev = netCDF4.Dataset(resource_directory + 'miroc_rmssdn.nc')

# #First half vs second half of MIROC
# output_filename = 'miroc_h1h2_IBAs.csv'
# figure_basename = 'miroc_h1h2'
# avg =     netCDF4.Dataset(resource_directory + 'miroc_h1_average.nc')
# stddev =  netCDF4.Dataset(resource_directory + 'miroc_h1_rmssdn.nc')
# pavg =    netCDF4.Dataset(resource_directory + 'miroc_h2_average.nc')
# pstddev = netCDF4.Dataset(resource_directory + 'miroc_h2_rmssdn.nc')

# #These are ECHOG vs CORE
# output_filename = 'echog_core_IBAs.csv'
# figure_basename = 'echog_core'
# avg =     netCDF4.Dataset(resource_directory + 'core_average.nc')
# stddev =  netCDF4.Dataset(resource_directory + 'core_rmssdn.nc')
# pavg =    netCDF4.Dataset(resource_directory + 'echog_average.nc')
# pstddev = netCDF4.Dataset(resource_directory + 'echog_rmssdn.nc')

# #First half vs second half of ECHOG
# output_filename = 'echog_h1h2_IBAs.csv'
# figure_basename = 'echog_h1h2'
# avg =     netCDF4.Dataset(resource_directory + 'echog_h1_average.nc')
# stddev =  netCDF4.Dataset(resource_directory + 'echog_h1_rmssdn.nc')
# pavg =    netCDF4.Dataset(resource_directory + 'echog_h2_average.nc')
# pstddev = netCDF4.Dataset(resource_directory + 'echog_h2_rmssdn.nc')

#Get a mask from the first variable
sample = ma.masked_array(avg.variables[metadata.orgName[0]][:])
mask=ma.getmask(sample)


# In[163]:

#This cell defines a plotting scheme that we can use to make consistent plots.

cmap = 'bwr'#'gist_heat'
origin = 'lower'
interpolation = 'nearest'
orientation = 'vertical'
shrink = 0.75

def create_subplot(data, pltnum, title, unitlabel):
    bound = np.max(np.absolute([np.min(data), np.max(data)]))
    vmin = -bound
    vmax = bound
    
    plt.subplot(1,2,pltnum)
    plt.title(title)
    img = plt.imshow(data, vmin=vmin, vmax=vmax, 
               origin=origin, interpolation=interpolation, cmap = cmap)
#    cm.gist_heat.set_bad('black', 0.8) # Set masked values to dark gray
    cb = plt.colorbar(img, orientation=orientation, extend='both', shrink=shrink, label=unitlabel)
    cm.bwr.set_bad('black', 0.8) # Set masked values to dark gray
    cb.locator = ticker.MaxNLocator(nbins=8)
    cb.update_ticks()
    return img


# In[164]:

#This loop goes through each variable, pulls out the mean and standard deviation for the
#projected and hindcasts models, and plots the differences between the two time periods.

for i in range(len(metadata['name'])):
    #Get a variable, just to see how many dimensions it has.
    hind_avg = ma.squeeze(ma.masked_array(avg.variables[metadata.orgName[i]][:]))
    surface_flag = 0
    
    if len(hind_avg.shape) > 2: # this takes just the surface value
        hind_avg =    ma.squeeze(ma.masked_array(avg.variables[metadata.orgName[i]][:])[0,0])
        hind_stddev = ma.squeeze(ma.masked_array(stddev.variables[metadata.orgName[i]][:])[0,0])
        proj_avg =    ma.squeeze(ma.masked_array(pavg.variables[metadata.orgName[i]][:])[0,0])
        proj_stddev = ma.squeeze(ma.masked_array(pstddev.variables[metadata.orgName[i]][:])[0,0])
        surface_flag = 1
    else:
        hind_stddev = ma.squeeze(ma.masked_array(stddev.variables[metadata.orgName[i]][:])[0])
        proj_avg =    ma.squeeze(ma.masked_array(pavg.variables[metadata.orgName[i]][:])[0])
        proj_stddev = ma.squeeze(ma.masked_array(pstddev.variables[metadata.orgName[i]][:])[0])
    #Create subplots
    fig = plt.figure(figsize=(16,5))
    if surface_flag == 1: plt.suptitle(metadata.name[i] + ', Surface Only', fontsize=20)
    else: plt.suptitle(metadata.name[i], fontsize=20)
    create_subplot(proj_avg-hind_avg, 1, 'Projected Mean - Current Mean', metadata.units[i])
    create_subplot(proj_stddev-hind_stddev, 2, 'Projected StdDev - Current StdDev', metadata.units[i])
    
    # Uncomment this line and change the output directory to save to files
    fig.savefig(output_directory + figure_basename + '_' + metadata.orgName[i],dpi=200)

    plt.show()


# ## Subset Models by IBAs
# Some shapes will be empty (i.e., there's a polygon, but no model data), others will have very few values. Some polygons over the shelf will have data at shallow bins, but not in the deeper ones. For the combined IBAs, there are 210 polygons, identified by sitenames, ids, and their geometries.

# In[165]:

# Load latitude and longitude arrays
latitude = np.array(avg.variables['LATITUDE'])
longitude = np.array(avg.variables['LONGITUDE'])

# Set up depth bins, for these data, they are in meters
depthbins = pd.DataFrame({'mindepth': [0, 75],
                          'maxdepth': [60, 200]},
                         index=['shallow','deep'],
                         columns=['mindepth', 'maxdepth', 'minz', 'maxz'])

depth = np.array(avg.variables['zsalt'])
depthindices=list()
for d in range(len(depthbins.index)):
    indicesz=np.where(np.logical_and(depth[:] <= depthbins.maxdepth[d],
                                     depth[:] >= depthbins.mindepth[d]))
    depthindices.append(indicesz[0])
    if len(depthindices[d]) > 0:
        depthbins.minz[d] = min(depthindices[d])
        depthbins.maxz[d] = max(depthindices[d])
        
depthbins['label']=['0 to 60 m', '75 to 200 m']


# In[166]:

#Create a dataframe to house the IBA site information, indexed by the IBA sitenames.
siteinfo = pd.DataFrame(
                        {
                        'id':ids,
                        'profile': profiles,
                        'minlat': minlat,
                        'minlon': minlon,
                        'maxlat': maxlat,
                        'maxlon': maxlon,
                        'tested': (['no'] * nsites)
                        },
                        index = sitenames
                       )

#We'll also add one more row, for the whole area, as a reference
areainfo = pd.DataFrame(
                        {
                        'id': 'None',
                        'profile': 'None',
                        'minlat': min(latitude),
                        'minlon': min(longitude),
                        'maxlat': max(latitude),
                        'maxlon': max(longitude),
                        'miny': [0],
                        'minx': [0],
                        'maxy': [len(latitude)],
                        'maxx': [len(longitude)],
                        'tested': ['yes']
                        },
                        index = ['Whole Area']
                       )

siteinfo=siteinfo.append(areainfo)
siteinfo=siteinfo[['id', 'profile', 'minlat', 'maxlat', 'minlon', 'maxlon', 'miny', 'maxy', 'minx', 'maxx', 'tested']]
nsites=nsites+1


# Now convert the latlons to pixel indices for the model.

# In[167]:

latindices = list()
lonindices = list()

for x in range(nsites):
    #first do latitude
    indicesy = np.where(np.logical_and(latitude <= siteinfo.maxlat[x],
                                       latitude >= siteinfo.minlat[x]))
    latindices.append(indicesy[0])
    #then longitude
    #PMEL models are in positive east, so we have to convert our bounding box.
    indicesx = np.where(np.logical_and(longitude[:] <= siteinfo.maxlon[x],
                                       longitude[:] >= siteinfo.minlon[x]))
    lonindices.append(indicesx[0])

    #finds the minimum and maxium pixel indices for each sitename
avgdata = ma.masked_array(avg.variables[metadata.orgName[0]][:])
for x in range(nsites-1): # -1 because we have on site that already has pixel indices
    if len(latindices[x]) > 0:
        siteinfo.miny[x] = min(latindices[x])
        siteinfo.maxy[x] = max(latindices[x])
    if len(lonindices[x]) > 0:
        siteinfo.minx[x] = min(lonindices[x])
        siteinfo.maxx[x] = max(lonindices[x]) 
#This tests to see if there are any values that are not NaNs. If there are any valid
#values, we'll set tested to "yes".
    if pd.notnull(siteinfo.miny[x]):
        if pd.notnull(siteinfo.maxy[x]):
            if pd.notnull(siteinfo.minx[x]):
                if pd.notnull(siteinfo.maxx[x]):
                    #This will find if there are any non-masked values to test.
                    count = ma.count(avgdata[0, siteinfo.miny[x]:siteinfo.maxy[x]+1, siteinfo.minx[x]:siteinfo.maxx[x]+1])
                    if count > 0:
                        siteinfo.tested[x] = 'yes'


# ## Process the data into a list of dataframes, indexed by sitename
# Now that we have the pixel mins and maxes, we can iterate through the variables:
# 
# 1. Check for depth: if present iterate three times for depth bins, if absent, just do once.  
# 2. Using the pixel indexes to subset each model variable for avg, stddev, pavg, and pstddev arrays and average those values.  
# 3. Create a data frame with the mean avg, stddev, pavg, and pstddev for each sitename, and append to list of dataframe results
# 4. Create a label including the variable name and depth bin, append to another list of results. There should be one label for each dataframe.
# 5. We'll concatenate these dataframes to the siteinfo dataframe into large dataframe for output to CSV.

# In[168]:

nvariables = len(metadata.orgName)
#create an empty dataframe
dfresults = pd.DataFrame(index = siteinfo.index)

#for each variable
for i in range(nvariables): 
    #Load in the data
    avgdata = ma.masked_array(avg.variables[metadata.orgName[i]][:])
    mask=ma.getmask(avgdata)
    stddevdata = ma.masked_array(stddev.variables[metadata.orgName[i]][:], mask)
    pavgdata = ma.masked_array(pavg.variables[metadata.orgName[i]][:], mask)
    pstddevdata = ma.masked_array(pstddev.variables[metadata.orgName[i]][:], mask)
    
    #check for depth
    if len(avgdata.shape) == 3:
        avgvals = list()
        stddevvals = list()
        pavgvals = list()
        pstddevvals = list()
        count = list()
        
        for x in range(nsites):
            if (siteinfo.tested[x] == 'yes'):
                #mean of data(latitude_indices, longitude_indices)
                #The +1 is necessary because numpy indexing goes from start:end, but end is the first value that's NOT included.
                #Whereas the way I've set this up, the maxy and maxx SHOULD be included.
                avgvals.append(ma.mean(avgdata[0, siteinfo.miny[x]:siteinfo.maxy[x]+1, siteinfo.minx[x]:siteinfo.maxx[x]+1]))
                stddevvals.append(ma.mean(stddevdata[0, siteinfo.miny[x]:siteinfo.maxy[x]+1, siteinfo.minx[x]:siteinfo.maxx[x]+1]))
                pavgvals.append(ma.mean(pavgdata[0, siteinfo.miny[x]:siteinfo.maxy[x]+1, siteinfo.minx[x]:siteinfo.maxx[x]+1]))
                pstddevvals.append(ma.mean(pstddevdata[0, siteinfo.miny[x]:siteinfo.maxy[x]+1, siteinfo.minx[x]:siteinfo.maxx[x]+1]))
                count.append(ma.count(avgdata[0, siteinfo.miny[x]:siteinfo.maxy[x]+1, siteinfo.minx[x]:siteinfo.maxx[x]+1]))

            else:
                avgvals.append(np.nan)
                stddevvals.append(np.nan)
                pavgvals.append(np.nan)
                pstddevvals.append(np.nan)
                count.append(np.nan)
        
        dfresults[metadata.name[i]+' Count'] = count
        dfresults[metadata.name[i]+' Avg'] = avgvals
        dfresults[metadata.name[i]+' StdDev'] = stddevvals
        dfresults[metadata.name[i]+' PAvg'] = pavgvals
        dfresults[metadata.name[i]+' PStdDEv'] = pstddevvals


    if len(avgdata.shape) == 4:
        for d in range(len(depthbins.index)):
            avgvals = list()
            stddevvals = list()
            pavgvals = list()
            pstddevvals = list()
            count = list()
            
            for x in range(nsites):
                
                if (siteinfo.tested[x] == 'yes'):
                    #mean of data(latitude_indices, longitude_indices)
                    #The +1 is necessary because numpy indexing goes from start:end, but end is the first value that's NOT included.
                    #Whereas the way I've set this up, the maxy and maxx SHOULD be included.
                    count.append(ma.count(avgdata[0, depthbins.minz[d]:depthbins.maxz[d]+1,
                              siteinfo.miny[x]:siteinfo.maxy[x]+1,
                              siteinfo.minx[x]:siteinfo.maxx[x]+1]))
                    if (count[x] > 0):
                        avgvals.append(ma.mean(avgdata[0, depthbins.minz[d]:depthbins.maxz[d]+1,
                                                       siteinfo.miny[x]:siteinfo.maxy[x]+1,
                                                       siteinfo.minx[x]:siteinfo.maxx[x]+1]))
                        stddevvals.append(ma.mean(stddevdata[0, depthbins.minz[d]:depthbins.maxz[d]+1,
                                                             siteinfo.miny[x]:siteinfo.maxy[x]+1,
                                                             siteinfo.minx[x]:siteinfo.maxx[x]+1]))
                        pavgvals.append(ma.mean(pavgdata[0, depthbins.minz[d]:depthbins.maxz[d]+1,
                                                         siteinfo.miny[x]:siteinfo.maxy[x]+1,
                                                         siteinfo.minx[x]:siteinfo.maxx[x]+1]))
                        pstddevvals.append(ma.mean(pstddevdata[0, depthbins.minz[d]:depthbins.maxz[d]+1,
                                                               siteinfo.miny[x]:siteinfo.maxy[x]+1,
                                                               siteinfo.minx[x]:siteinfo.maxx[x]+1]))
                    else:
                        avgvals.append(np.nan)
                        stddevvals.append(np.nan)
                        pavgvals.append(np.nan)
                        pstddevvals.append(np.nan)
                else:
                    avgvals.append(np.nan)
                    stddevvals.append(np.nan)
                    pavgvals.append(np.nan)
                    pstddevvals.append(np.nan)
                    count.append(np.nan)
                    
            dfresults[metadata.name[i]+' '+depthbins.label[d]+' Count'] = count
            dfresults[metadata.name[i]+' '+depthbins.label[d]+' Avg' + ' in ' + metadata.units[i]] = avgvals
            dfresults[metadata.name[i]+' '+depthbins.label[d]+' StdDev' + ' in ' + metadata.units[i]] = stddevvals
            dfresults[metadata.name[i]+' '+depthbins.label[d]+' PAvg' + ' in ' + metadata.units[i]] = pavgvals
            dfresults[metadata.name[i]+' '+depthbins.label[d]+' PStdDEv' + ' in ' + metadata.units[i]] = pstddevvals


# In[169]:

#This looks at one site, just to test that we're getting some values
i = 18
print siteinfo.iloc[i]
dfresults.iloc[i,0:10]


# ## Export to CSV
# Combine the siteinfo and dfresults dataframes, and write out to CSV.

# In[170]:

# merged = siteinfo.join(dfresults, sort=False)
# merged.to_csv(output_directory + output_filename, index_label='sitenames')


# ## Interested in single species core areas instead of combined IBAs?
# The single-species core area polygons have different metadata and variable names, and are handled a bit differently in [this notebook](subset_higher_stats_byspecies.ipynb).
# 
