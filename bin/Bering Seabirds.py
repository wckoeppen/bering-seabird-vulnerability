# -*- coding: utf-8 -*-
# <nbformat>3.0</nbformat>

# <markdowncell>

# #Bering Seabirds and Future Climate Variability
# Assessing the projected change in climate variability of areas used by seabirds in the Bering Sea. This notebook is a self-contained (and well documented) code base to subset climate projections stored as NetCDF files by Important Bird Area polygons (hosted by the Alaska Ocean Observing System).

# <codecell>

#A list of imports we need for code later in the notebook.
#The css_styles() function must go last.
%matplotlib inline
from owslib.wfs import WebFeatureService
import json
from utilities import find_dict_keys
from shapely.geometry import shape, MultiPolygon
from shapely.geometry import box

import folium
from utilities import get_coords
from IPython.core.display import HTML

import time
import numpy as np
from numpy import ma
import netCDF4
import pandas as pd
from pandas import Series

import matplotlib as mpl
from matplotlib import cm
import matplotlib.pyplot as plt
from matplotlib import ticker

from utilities import css_styles
css_styles()

# <markdowncell>

# ##Load Important Bird Areas
# These are defined by [Audobon](http://web4.audubon.org/bird/iba/) and hosted by AOOS on a geoserver. Currently, they represent areas important for numerous species of birds.
# 
# The single species core areas must be processed differently, because they have different metadata and include species information. They are contained in a separate notebook: [located here](http://127.0.0.1:8888/cc62f4c2-b95f-41a4-9b84-13c46ce9c37a).

# <codecell>

#Load our important bird areas again.
known_wfs = "http://solo.axiomalaska.com/geoserver/audubon_ibav3/ows"
wfs = WebFeatureService(known_wfs, version='1.0.0')
geojson_response = wfs.getfeature(typename=['audubon_ibav3:audubon_ibas_v3_20aug2014'], outputFormat="application/json", srsname="urn:x-ogc:def:crs:EPSG:4326").read()
geojson = json.loads(geojson_response)

geometries = find_dict_keys('geometry', geojson)
shapes = [shape(g) for g in geometries]

ids = find_dict_keys('id', geojson)
ids=[str(j) for j in ids]

sitenums = find_dict_keys('newsitenam', geojson)
sitenums = [str(s) for s in sitenums]
nsites=len(sitenums)

#This generates bounding boxes from the complex geometries in shapes.
minlat = list()
minlon = list()
maxlat = list()
maxlon = list()
for s in shapes:
    minlat.append(s.bounds[0])
    minlon.append(s.bounds[1])
    maxlat.append(s.bounds[2])
    maxlon.append(s.bounds[3])
# miny, minx, maxy, maxx

# <codecell>

#Longitudes in the model are in positive east.
#Longitudes in the shape files are -180/180, which I'll convert to the model coordinates

for x in range(nsites):
    if minlon[x] < 0:
        minlon[x] = minlon[x]+360.
    if maxlon[x] < 0:
        maxlon[x] = maxlon[x]+360.

# <markdowncell>

# For now, this just uses a bounding box for each IBA rather than polygons. Because actual polygons are hard. Note: don't worry about the wrapped boxes in the display. Those are fixed in reality (just above), but we're plotting from unfixed data.

# <codecell>

mapper = folium.Map(location=[65.1, -150.6], zoom_start=4)
#Mapy polygons in red
for s in shapes:
    for c in get_coords(s):
        mapper.line(c, line_color='#FF0000', line_weight=2)

#Map bounding boxes of polygons in blue
for s in shapes:
# miny, minx, maxy, maxx
    analysis_box = s.bounds
    bound = box(*analysis_box).boundary.coords
    mapper.line(bound, line_color='#0000FF', line_weight=3)

mapper.lat_lng_popover()
mapper._build_map()
HTML('<iframe srcdoc="{srcdoc}" style="width:100%; height: 535px; border: none"></iframe>'.format(srcdoc=mapper.HTML.replace('"', '&quot;')))

# <markdowncell>

# ## Process model data
# ###Set Climate basline model from which to pull data
# Initially we were going to use the [PMEL Co-ordinated Ocean-Ice Reference Experiments (CORE) Climate Model](http://portal.aoos.org/?v=rand&portal_id=25#module-metadata/5626a0b6-7d79-11e3-ac17-00219bfe5678/82fde9d7-ccdf-4404-a9bd-c159d9d6461d) - 01/19/1969 - 01/02/2005  
# as our baseline time-period. In theory, we have two other models which have data over our baseline period, [CFS-R](http://portal.aoos.org/?v=rand&portal_id=25#module-metadata/f8cb79f6-7d59-11e3-a6ee-00219bfe5678/af7b2758-e7d0-4d82-96eb-1696c464586d) and [CFSv2](http://portal.aoos.org/?v=rand&portal_id=25#module-metadata/4e77fed6-7d7a-11e3-97d7-00219bfe5678/0e50760b-8795-4aac-a155-dd624aff9c24), but we didn't recieve the full time-series for either of these models.
# 
# However, then we realized there is model bias that we can't account for because we have no model data that overlaps with the CORE time frame.
# 
# ###Climate projections(s) from which to pull data
# [PMEL CCCma](http://portal.aoos.org/alaska-statewide.php#module-metadata/4f706756-7d57-11e3-bce5-00219bfe5678/6b2d1f96-4c5b-4b13-ad57-c8ca2e326a34) - 01/26/2003 - 01/01/2040  
# [PMEL MIROC](http://portal.aoos.org/alaska-statewide.php#module-metadata/68ea728a-7d7a-11e3-823b-00219bfe5678/50242dc9-b639-42c4-8287-d0aeb082fb1f) - 12/29/2002 - 12/04/2039  
# [PMEL ECHO-G](http://portal.aoos.org/alaska-statewide.php#module-metadata/18ffa59c-7d7a-11e3-82a4-00219bfe5678/7bfa82c2-3051-484e-8c7b-9b9f711083f5) - 12/29/2002 - 12/04/2039  
# <div class="warning"><strong>CCCma Model Dates</strong> - CCCma models have slightly different dates. This is because the modeling groups started at slightly different times. Nick Bond says these dates should not be rounded (in order to make them all match up) as it will increase model error.</div>
# 
# Each forecast model produced one time slice per week from 2002 to 2040. Loading these models remotely worked, but they are too large to do analyses over the web. I downloaded these files to my local machine via THREDDS (~294 Gb of files for each model). Unfortunately, by doing the work locally, we lost the advantage of using the ncml virtual aggregated datasets (which makes the whole lot of files looks like a single dataset).Â We also cannot use any of the metadata improvements that AOOS makes using ncml.
# 
# ###Actual Analysis
# Because we have no measure of bias, the best we can do is compare one model to itself. So we compared the first half of the CCCma model (model weeks 1001-1964, or Jan 26, 2003 - July 11, 2021) to the second half (model weeks 2965 - 2928, or July 18, 2021 - Jan 01, 2014). These are creatively named H1 and H2, respectively.
# 
# ###Processing using NCO
# The model results are quite large, and we have to think a lot about memory management when trying to process the files and calculate statistics. I tried doing it in python, but calculated that the processing would take over 2 months of CPU time. Whereas it takes a few hours using an external package of binaries.
# 
# This uses a series of binary functions that are part of the [NetCDF Operator (NCO)](http://nco.sourceforge.net/) suite. NCO programs are standalone programs that must be installed and run separately. They are documented here for completeness.
# 
# We are trying to calculate first the mean:
# <pre><code>
# FORM
# ncea input_files output_file
# -y ttl: calculates the total of all values
# -y sqravg: calculates the average of all the squares
# 
# </code></pre>
# Once we have the average, we can calculate the differences from the mean using NCBO. This takes the first file, subtracts the second file, and writes to the third file. In our case, we are creating a series of files that represent the difference from the mean.
# <pre><code>
# FORM
# ncbo input_file1 input_file2 output_file
# </code></pre>
# NCBO can only work on one file at a time (it doesn't accept a list of files), so I wrote a bash script that loops through and does an NCBO command for every file in the listed directory.
# 
# NCEA and NCBO are smart enough to use and preserve the variable names in each NetCDF file, so the resulting file will have a processed cube of data for all the variables already. It even preserves the Z column (so only averaging through time, not space). Once we have directory full of anomalies (differences), we NCEA again to calculate the root mean square of the differences (i.e., the standard deviation).
# 
# <pre><code>
# FORM
# ncea -y rmssdn input_files, outputfile
# -y rmssdn: calculates the root mean square of the differences.
# </code></pre>
# 
# ####First Half
# #####NCO
# <pre><code>
# ncea /augie/gluster/data/netCDF/pmel/cccma/cccma_week{1001..1964}.nc /augie/gluster/data/netCDF/pmel/cccma/Outputs/cccma_average_h1.nc
# </code></pre>
# 
# #####Bash script, made executable and run locally
# <pre><code>
# #!/bin/bash
# count=1001
# while [ $count -le 1964 ]
# do
#     infile="/augie/gluster/data/netCDF/pmel/cccma/cccma_week${count}.nc"
#     mean="/augie/gluster/data/netCDF/pmel/cccma/Outputs/cccma_average_h1.nc"
#     outfile="/augie/gluster/data/netCDF/pmel/cccma/Diff/cccma_diffh1_week${count}.nc"
#     echo "Executing: ncbo $infile $mean $outfile"
#     ncbo $infile $mean $outfile
#     let count=count+1
# done
# </code></pre>
# 
# #####NCO
# <pre><code>
# ncea -y rmssdn /augie/gluster/data/netCDF/pmel/cccma/Diff/cccma_diffh1_week*.nc /augie/gluster/data/netCDF/pmel/cccma/Outputs/cccma_h1_rmssdn.nc
# </code></pre>
# 
# ####Second Half
# #####NCO
# <pre><code>
# ncea /augie/gluster/data/netCDF/pmel/cccma/cccma_week{1965..2928}.nc /augie/gluster/data/netCDF/pmel/cccma/Outputs/cccma_average_h2.nc
# </code></pre>
# 
# #####Bash script, made executable and run locally
# <pre><code>
# #!/bin/bash
# count=1965
# while [ $count -le 2928 ]
# do
#     infile="/augie/gluster/data/netCDF/pmel/cccma/cccma_week${count}.nc"
#     mean="/augie/gluster/data/netCDF/pmel/cccma/Outputs/cccma_average_h2.nc"
#     outfile="/augie/gluster/data/netCDF/pmel/cccma/Diff/cccma_diffh2_week${count}.nc"
#     echo "Executing: ncbo $infile $mean $outfile"
#     ncbo $infile $mean $outfile
#     let count=count+1
# done
# </code></pre>
# 
# #####NCO
# <pre><code>
# ncea -y rmssdn /augie/gluster/data/netCDF/pmel/cccma/Diff/cccma_diffh2_week*.nc /augie/gluster/data/netCDF/pmel/cccma/Outputs/cccma_h2_rmssdn.nc
# </code></pre>

# <markdowncell>

# ##Load Processed Model Results
# These statistics outputs are loaded in, so we can do our statistics. Because we're accessing raw NetCDF files instead of aggregated and curated NcML, we need to access the data using the original variable names. These can be found using ncdump, or some other netCDF software. I'm loading these into a panda DataFrame.

# <codecell>

#The original metadata was not very good, and not all fields were filled in (e.g., units),
#so we have to correct these. This is our known metadata for the outputs:
metadata = pd.DataFrame({
         'orgName': [
                   #these don't have depth
                   'icephl_latlon','ben_latlon',
                   'aice_latlon',
                   #these do
                 'phs_latlon','phl_latlon','mzl_latlon','cop_latlon','ncao_latlon','ncas_latlon','eup_latlon','det_latlon',
                 'temp_latlon',
                 'u_latlon',
                 'v_latlon',],
         'name': ['Ice Phytoplankton Concentration',
                  'Benthos Concentration',
                  'Sea Ice Area Fraction',
                  'Small Phytoplankton Concentration',
                  'Large Phytoplankton Concentration',
                  'Large Microzooplankton Concentration',
                  'Small Coastal Copepod Concentration',
                  'Offshore Neocalanus Concentration',
                  'Neocalanus Concentration',
                  'Euphausiids Concentration',
                  'Detritus Concentration',
                  'Sea Water Temperature',
                  'Zonal (U) Current',
                  'Meridional (V) Current'],
         'units': ['mgC/m2','mgC/m2',
                   'Fraction',
                   'mgC/m3','mgC/m3','mgC/m3','mgC/m3','mgC/m3','mgC/m3','mgC/m3','mgC/m3',
                   'degrees C',
                   'm/s',
                   'm/s']
       })

# <codecell>

#Hindcast
#These are the H1/H2 stats of the CCCma model
#NOTE: in the respository, these files are gzipped and must be gunzipped before this cell will work.
avg =     netCDF4.Dataset('~/Projects/bering-seabird-vulnerability/Resources/cccma_h1_average.nc')
stddev =  netCDF4.Dataset('~/Projects/bering-seabird-vulnerability/Resources/cccma_h1_rmssdn.nc')
pavg =    netCDF4.Dataset('~/Projects/bering-seabird-vulnerability/Resources/cccma_h2_average.nc')
pstddev = netCDF4.Dataset('~/Projects/bering-seabird-vulnerability/Resources/cccma_h2_rmssdn.nc')

#These were the previous stats
#avg = netCDF4.Dataset('~/Projects/bering-seabird-vulnerability/Resources/core_average.nc')
#stddev = netCDF4.Dataset('~/Projects/bering-seabird-vulnerability/Resources/core_rmssdn.nc')
#pavg = netCDF4.Dataset('~/Projects/bering-seabird-vulnerability/Resources/cccma_average.nc')
#pstddev = netCDF4.Dataset('~/Projects/bering-seabird-vulnerability/Resources/cccma_rmssdn.nc')

#Get a mask from the first variable
sample = ma.masked_array(avg.variables[metadata.orgName[0]][:])
mask=ma.getmask(sample)

# <codecell>

#This cell defines a plotting scheme that we can use to make consistent plots.

cmap = 'bwr'#'gist_heat'
origin = 'lower'
interpolation = 'nearest'
orientation = 'vertical'
shrink = 0.75

def create_subplot(data, pltnum, title, unitlabel):
    bound = np.max(np.absolute([np.min(data), np.max(data)]))
    vmin = -bound
    vmax = bound
    
    plt.subplot(1,2,pltnum)
    plt.title(title)
    img = plt.imshow(data, vmin=vmin, vmax=vmax, 
               origin=origin, interpolation=interpolation, cmap = cmap)
#    cm.gist_heat.set_bad('black', 0.8) # Set masked values to dark gray
    cb = plt.colorbar(img, orientation=orientation, extend='both', shrink=shrink, label=unitlabel)
    cm.bwr.set_bad('black', 0.8) # Set masked values to dark gray
    cb.locator = ticker.MaxNLocator(nbins=8)
    cb.update_ticks()
    return img

# <codecell>

#This loop goes through each variable, pulls out the mean and standard deviation for the
#projected and hindcasts models, and plots the differences between the two time periods.

for i in range(len(metadata['name'])):
    #Get a variable, just to see how many dimensions it has.
    hind_avg = ma.squeeze(ma.masked_array(avg.variables[metadata.orgName[i]][:]))
    surface_flag = 0
    
    if len(hind_avg.shape) > 2: # this takes just the surface value
        hind_avg =    ma.squeeze(ma.masked_array(avg.variables[metadata.orgName[i]][:])[0,0])
        hind_stddev = ma.squeeze(ma.masked_array(stddev.variables[metadata.orgName[i]][:])[0,0])
        proj_avg =    ma.squeeze(ma.masked_array(pavg.variables[metadata.orgName[i]][:])[0,0])
        proj_stddev = ma.squeeze(ma.masked_array(pstddev.variables[metadata.orgName[i]][:])[0,0])
        surface_flag = 1
    else:
        hind_stddev = ma.squeeze(ma.masked_array(stddev.variables[metadata.orgName[i]][:])[0])
        proj_avg =    ma.squeeze(ma.masked_array(pavg.variables[metadata.orgName[i]][:])[0])
        proj_stddev = ma.squeeze(ma.masked_array(pstddev.variables[metadata.orgName[i]][:])[0])
    #Create subplots
    fig = plt.figure(figsize=(16,5))
    if surface_flag == 1: plt.suptitle(metadata.name[i] + ', Surface Only', fontsize=20)
    else: plt.suptitle(metadata.name[i], fontsize=20)
    create_subplot(proj_avg-hind_avg, 1, 'Projected Mean - Historical Mean', metadata.units[i])
    create_subplot(proj_stddev-hind_stddev, 2, 'Projected StdDev - Historical StdDev', metadata.units[i])
    
    # Uncomment this line and change the output directory to save to files
    #fig.savefig('/home/will/Desktop/' + metadata.orgName[i],dpi=200)

    plt.show()

# <markdowncell>

# ##Subset Models by IBAs
# Some shapes will be empty (i.e., there's a polygon, but no model data), others will have very few values. Some polygons over the shelf will have data at shallow bins, but not in the deeper ones. For the combined IBAs, there are 210 polygons, identified by sitenums, objectids, and their geometries.

# <codecell>

# Load latitude and longitude arrays
latitude = np.array(avg.variables['LATITUDE'])
longitude = np.array(avg.variables['LONGITUDE'])

# Set up depth bins, for these data, they are in meters

depthbins = pd.DataFrame({'mindepth': [0, 10, 75],
                          'maxdepth': [5, 60, 200]},
                         index=['surface', 'midwater','deepwater'],
                         columns=['mindepth', 'maxdepth', 'minz', 'maxz'])

depth = np.array(avg.variables['zsalt'])
depthindices=list()
for d in range(len(depthbins.index)):
    indicesz=np.where(np.logical_and(depth[:] <= depthbins.maxdepth[d],
                                     depth[:] >= depthbins.mindepth[d]))
    depthindices.append(indicesz[0])
    if len(depthindices[d]) > 0:
        depthbins.minz[d] = min(depthindices[d])
        depthbins.maxz[d] = max(depthindices[d])
        
depthbins['label']=['0 to 5 m', '10 to 60 m', '75 to 200 m']

# <codecell>

#Create a dataframe to house the IBA site information, indexed by the IBA sitenums.
siteinfo = pd.DataFrame(
                        {
                        'id':ids,
                        'minlat': minlat,
                        'minlon': minlon,
                        'maxlat': maxlat,
                        'maxlon': maxlon,
                        'tested': (['no'] * nsites)
                        },
                        index = sitenums
                       )

#We'll also add one more row, for the whole area, as a reference
areainfo = pd.DataFrame(
                        {
                        'id': 'None',
                        'minlat': min(latitude),
                        'minlon': min(longitude),
                        'maxlat': max(latitude),
                        'maxlon': max(longitude),
                        'miny': [0],
                        'minx': [0],
                        'maxy': [len(latitude)],
                        'maxx': [len(longitude)],
                        'tested': ['yes']
                        },
                        index = ['Whole Area']
                       )

siteinfo=siteinfo.append(areainfo)
siteinfo=siteinfo[['id', 'minlat', 'maxlat', 'minlon', 'maxlon', 'miny', 'maxy', 'minx', 'maxx', 'tested']]
nsites=nsites+1

# <markdowncell>

# Now convert the latlons to pixel indices for the model.

# <codecell>

latindices = list()
lonindices = list()

for x in range(nsites):
    #first do latitude
    indicesy = np.where(np.logical_and(latitude <= siteinfo.maxlat[x],
                                       latitude >= siteinfo.minlat[x]))
    latindices.append(indicesy[0])
    #then longitude
    #PMEL models are in positive east, so we have to convert our bounding box.
    indicesx = np.where(np.logical_and(longitude[:] <= siteinfo.maxlon[x],
                                       longitude[:] >= siteinfo.minlon[x]))
    lonindices.append(indicesx[0])

    #finds the minimum and maxium pixel indices for each sitenum
avgdata = ma.masked_array(avg.variables[metadata.orgName[0]][:])
for x in range(nsites-1):
    if len(latindices[x]) > 0:
        siteinfo.miny[x] = min(latindices[x])
        siteinfo.maxy[x] = max(latindices[x])
    if len(lonindices[x]) > 0:
        siteinfo.minx[x] = min(lonindices[x])
        siteinfo.maxx[x] = max(lonindices[x]) 
#This tests to see if there are any values that are not NaNs. If there are any valid
#values, we'll set tested to "yes".
    if pd.notnull(siteinfo.miny[x]):
        if pd.notnull(siteinfo.maxy[x]):
            if pd.notnull(siteinfo.minx[x]):
                if pd.notnull(siteinfo.maxx[x]):
                    #This will find if there are any non-masked values to test.
                    count = ma.count(avgdata[0, siteinfo.miny[x]:siteinfo.maxy[x]+1, siteinfo.minx[x]:siteinfo.maxx[x]+1])
                    if count > 0:
                        siteinfo.tested[x] = 'yes'

# <markdowncell>

# ##Process the data into a list of dataframes, indexed by sitenum
# Now that we have the pixel mins and maxes, we can iterate through the variables:
# 
# 1. Check for depth: if present iterate three times for depth bins, if absent, just do once.  
# 2. Using the pixel indexes to subset each model variable for avg, stddev, pavg, and pstddev arrays and average those values.  
# 3. Create a data frame with the mean avg, stddev, pavg, and pstddev for each sitenum, and append to list of dataframe results
# 4. Create a label including the variable name and depth bin, append to another list of results. There should be one label for each dataframe.
# 5. We'll concatenate these dataframes to the siteinfo dataframe into large dataframe for output to CSV.

# <codecell>

nvariables = len(metadata.orgName)
#create an empty dataframe
dfresults = pd.DataFrame(index = siteinfo.index)

#for each variable
for i in range(nvariables): 
    #Load in the data
    avgdata = ma.masked_array(avg.variables[metadata.orgName[i]][:])
    mask=ma.getmask(avgdata)
    stddevdata = ma.masked_array(stddev.variables[metadata.orgName[i]][:], mask)
    pavgdata = ma.masked_array(pavg.variables[metadata.orgName[i]][:], mask)
    pstddevdata = ma.masked_array(pstddev.variables[metadata.orgName[i]][:], mask)
    
    #check for depth
    if len(avgdata.shape) == 3:
        avgvals = list()
        stddevvals = list()
        pavgvals = list()
        pstddevvals = list()
        count = list()
        
        for x in range(nsites):
            if (siteinfo.tested[x] == 'yes'):
                #mean of data(latitude_indices, longitude_indices)
                #The +1 is necessary because numpy indexing goes from start:end, but end is the first value that's NOT included.
                #Whereas the way I've set this up, the maxy and maxx SHOULD be included.
                avgvals.append(ma.mean(avgdata[0, siteinfo.miny[x]:siteinfo.maxy[x]+1, siteinfo.minx[x]:siteinfo.maxx[x]+1]))
                stddevvals.append(ma.mean(stddevdata[0, siteinfo.miny[x]:siteinfo.maxy[x]+1, siteinfo.minx[x]:siteinfo.maxx[x]+1]))
                pavgvals.append(ma.mean(pavgdata[0, siteinfo.miny[x]:siteinfo.maxy[x]+1, siteinfo.minx[x]:siteinfo.maxx[x]+1]))
                pstddevvals.append(ma.mean(pstddevdata[0, siteinfo.miny[x]:siteinfo.maxy[x]+1, siteinfo.minx[x]:siteinfo.maxx[x]+1]))
                count.append(ma.count(avgdata[0, siteinfo.miny[x]:siteinfo.maxy[x]+1, siteinfo.minx[x]:siteinfo.maxx[x]+1]))

            else:
                avgvals.append(np.nan)
                stddevvals.append(np.nan)
                pavgvals.append(np.nan)
                pstddevvals.append(np.nan)
                count.append(np.nan)
        
        dfresults[metadata.name[i]+' Count'] = count
        dfresults[metadata.name[i]+' Avg'] = avgvals
        dfresults[metadata.name[i]+' StdDev'] = stddevvals
        dfresults[metadata.name[i]+' PAvg'] = pavgvals
        dfresults[metadata.name[i]+' PStdDEv'] = pstddevvals


    if len(avgdata.shape) == 4:
        for d in range(len(depthbins.index)):
            avgvals = list()
            stddevvals = list()
            pavgvals = list()
            pstddevvals = list()
            count = list()
            
            for x in range(nsites):
                
                if (siteinfo.tested[x] == 'yes'):
                    #mean of data(latitude_indices, longitude_indices)
                    #The +1 is necessary because numpy indexing goes from start:end, but end is the first value that's NOT included.
                    #Whereas the way I've set this up, the maxy and maxx SHOULD be included.
                    count.append(ma.count(avgdata[0, depthbins.minz[d]:depthbins.maxz[d]+1,
                              siteinfo.miny[x]:siteinfo.maxy[x]+1,
                              siteinfo.minx[x]:siteinfo.maxx[x]+1]))
                    if (count[x] > 0):
                        avgvals.append(ma.mean(avgdata[0, depthbins.minz[d]:depthbins.maxz[d]+1,
                                                       siteinfo.miny[x]:siteinfo.maxy[x]+1,
                                                       siteinfo.minx[x]:siteinfo.maxx[x]+1]))
                        stddevvals.append(ma.mean(stddevdata[0, depthbins.minz[d]:depthbins.maxz[d]+1,
                                                             siteinfo.miny[x]:siteinfo.maxy[x]+1,
                                                             siteinfo.minx[x]:siteinfo.maxx[x]+1]))
                        pavgvals.append(ma.mean(pavgdata[0, depthbins.minz[d]:depthbins.maxz[d]+1,
                                                         siteinfo.miny[x]:siteinfo.maxy[x]+1,
                                                         siteinfo.minx[x]:siteinfo.maxx[x]+1]))
                        pstddevvals.append(ma.mean(pstddevdata[0, depthbins.minz[d]:depthbins.maxz[d]+1,
                                                               siteinfo.miny[x]:siteinfo.maxy[x]+1,
                                                               siteinfo.minx[x]:siteinfo.maxx[x]+1]))
                    else:
                        avgvals.append(np.nan)
                        stddevvals.append(np.nan)
                        pavgvals.append(np.nan)
                        pstddevvals.append(np.nan)
                else:
                    avgvals.append(np.nan)
                    stddevvals.append(np.nan)
                    pavgvals.append(np.nan)
                    pstddevvals.append(np.nan)
                    count.append(np.nan)
                    
            dfresults[metadata.name[i]+' '+depthbins.label[d]+' Count'] = count
            dfresults[metadata.name[i]+' '+depthbins.label[d]+' Avg' + ' in ' + metadata.units[i]] = avgvals
            dfresults[metadata.name[i]+' '+depthbins.label[d]+' StdDev' + ' in ' + metadata.units[i]] = stddevvals
            dfresults[metadata.name[i]+' '+depthbins.label[d]+' PAvg' + ' in ' + metadata.units[i]] = pavgvals
            dfresults[metadata.name[i]+' '+depthbins.label[d]+' PStdDEv' + ' in ' + metadata.units[i]] = pstddevvals

# <codecell>

#This looks at one site, just to test that we're getting some values
print siteinfo.iloc[2]
dfresults.iloc[2,0:10]

# <markdowncell>

# ## Export to CSV
# Combine the siteinfo and dfresults dataframes, and write out to CSV.

# <codecell>

merged = siteinfo.join(dfresults, sort=False)
merged.to_csv('statsIBAv3.csv', index_label='sitenum')

# <markdowncell>

# ##Visualize Higher Order Statistics
# Higher order stats will be calculated in [this notebook](calculate_higher_stats.ipynb).  
# We subset the higher order stats using our combined IBA polygons in [this notebook](subset_higher_stats.ipynb).  
# We subset the stats using our single-species core area polygons in [this notebook](subset_higher_stats_byspecies.ipynb).

# <codecell>


