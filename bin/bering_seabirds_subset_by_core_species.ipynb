{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subset higher order stats by our bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "        .info {\n",
       "            background-color: #fcf8e3; border-color: #faebcc; border-left: 5px solid #8a6d3b; padding: 0.5em; color: #8a6d3b;\n",
       "        }\n",
       "        .success {\n",
       "            background-color: #d9edf7; border-color: #bce8f1; border-left: 5px solid #31708f; padding: 0.5em; color: #31708f;\n",
       "        }\n",
       "        .error {\n",
       "            background-color: #f2dede; border-color: #ebccd1; border-left: 5px solid #a94442; padding: 0.5em; color: #a94442;\n",
       "        }\n",
       "        .warning {\n",
       "            background-color: #fcf8e3; border-color: #faebcc; border-left: 5px solid #8a6d3b; padding: 0.5em; color: #8a6d3b;\n",
       "        }\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#A list of imports we need for code later in the notebook.\n",
    "#The css_styles() function must go last.\n",
    "%matplotlib inline\n",
    "from owslib.wfs import WebFeatureService\n",
    "import json\n",
    "from utilities import find_dict_keys\n",
    "from shapely.geometry import shape, MultiPolygon\n",
    "from shapely.geometry import box\n",
    "\n",
    "import folium\n",
    "from utilities import get_coords\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "from numpy import ma\n",
    "import netCDF4\n",
    "import pandas as pd\n",
    "from pandas import Series\n",
    "\n",
    "import matplotlib as mpl\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker\n",
    "\n",
    "from utilities import css_styles\n",
    "css_styles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load our important bird areas again.\n",
    "known_wfs = \"http://solo.axiomalaska.com/geoserver/audubon_ibav3/ows\"\n",
    "wfs = WebFeatureService(known_wfs, version='1.0.0')\n",
    "geojson_response = wfs.getfeature(typename=['audubon_ibav3:audubon_ibas_v3_single_spp_core_areas_20aug2014'],\n",
    "                                  outputFormat=\"application/json\",\n",
    "                                  srsname=\"urn:x-ogc:def:crs:EPSG:4326\").read()\n",
    "geojson = json.loads(geojson_response)\n",
    "\n",
    "geometries = find_dict_keys('geometry', geojson)\n",
    "shapes = [shape(g) for g in geometries]\n",
    "\n",
    "sitenames = find_dict_keys('sitename', geojson)\n",
    "sitenames = [str(s) for s in sitenames]\n",
    "nsites=len(sitenames)\n",
    "\n",
    "iba_types = find_dict_keys('iba_type', geojson)\n",
    "iba_types = [str(s) for s in iba_types]\n",
    "\n",
    "profiles = find_dict_keys('profile', geojson)\n",
    "profiles = [str(s) for s in profiles]\n",
    "\n",
    "species = find_dict_keys('species', geojson)\n",
    "species = [str(s) for s in species]\n",
    "\n",
    "#This generates bounding boxes from the complex geometries in shapes.\n",
    "minlat = list()\n",
    "minlon = list()\n",
    "maxlat = list()\n",
    "maxlon = list()\n",
    "for s in shapes:\n",
    "    minlat.append(s.bounds[0])\n",
    "    minlon.append(s.bounds[1])\n",
    "    maxlat.append(s.bounds[2])\n",
    "    maxlon.append(s.bounds[3])\n",
    "# miny, minx, maxy, maxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#In the interest of time, we're going to jury-rig these.\n",
    "#The problem is that the shape files we were sent have a break at -180/180,\n",
    "#which is making everything freak out.\n",
    "\n",
    "# Aleutian Islands\n",
    "minlat.append(50.7)\n",
    "minlon.append(171.4)\n",
    "maxlat.append(53.5)\n",
    "maxlon.append(-168.8)\n",
    "profiles.append('None')\n",
    "sitenames.append('Aleutian Island LME')\n",
    "iba_types.append('NA')\n",
    "species.append('NA')\n",
    "\n",
    "# Bering Sea\n",
    "minlat.append(52.0)\n",
    "minlon.append(172.7)\n",
    "maxlat.append(61.4)\n",
    "maxlon.append(-160.1)\n",
    "profiles.append('None')\n",
    "sitenames.append('West Bering Sea LME')\n",
    "iba_types.append('NA')\n",
    "species.append('NA')\n",
    "\n",
    "# Chukchi Sea\n",
    "minlat.append(61.5)\n",
    "minlon.append(173.4)\n",
    "maxlat.append(79.0)\n",
    "maxlon.append(-155.6)\n",
    "profiles.append('None')\n",
    "sitenames.append('Chukchi Sea LME')\n",
    "iba_types.append('NA')\n",
    "species.append('NA')\n",
    "\n",
    "#Gulf of Alaska\n",
    "minlat.append(51.0)\n",
    "minlon.append(-164.0)\n",
    "maxlat.append(61.0)\n",
    "maxlon.append(-136.0)\n",
    "profiles.append('None')\n",
    "sitenames.append('Gulf of Alaska LME')\n",
    "iba_types.append('NA')\n",
    "species.append('NA')\n",
    "\n",
    "nsites = nsites + 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Longitudes in the model are in positive east.\n",
    "#Longitudes in the shape files are -180/180, which I'll convert to the model coordinates\n",
    "\n",
    "for x in range(nsites):\n",
    "    if minlon[x] < 0:\n",
    "        minlon[x] = minlon[x]+360.\n",
    "    if maxlon[x] < 0:\n",
    "        maxlon[x] = maxlon[x]+360.\n",
    "        \n",
    "#When I do this, some of the shapes that cross the dateline can seem to switch\n",
    "#We need to switch these back.\n",
    "for x in range(nsites):\n",
    "    if minlon[x] > maxlon[x]:\n",
    "        bigger = minlon[x]\n",
    "        smaller = maxlon[x]\n",
    "        maxlon[x]=bigger\n",
    "        minlon[x]=smaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#NOTE: in the respository, these files are gzipped and must be gunzipped before this cell will work.\n",
    "\n",
    "output_directory = '/home/will/Projects/bering-seabird-vulnerability/outputs/'\n",
    "resource_directory = '/home/will/Projects/bering-seabird-vulnerability/resources/'\n",
    "\n",
    "#(1) in the respository, these files are gzipped and must be gunzipped before this cell will work,\n",
    "#(2) change the filenames to match your system\n",
    "#(3) you need the full path, ipython doesn't seem to understand ~/.\n",
    "\n",
    "# CCCma vs Core\n",
    "output_filename = 'cccma_core_coreareas.csv'\n",
    "avg =     netCDF4.Dataset(resource_directory + 'core_average.nc')\n",
    "stddev =  netCDF4.Dataset(resource_directory + 'core_rmssdn.nc')\n",
    "pavg =    netCDF4.Dataset(resource_directory + 'cccma_average.nc')\n",
    "pstddev = netCDF4.Dataset(resource_directory + 'cccma_rmssdn.nc')\n",
    "\n",
    "# # First half vs second half of CCCma\n",
    "# output_filename = 'cccma_h1h2_coreareas.csv'\n",
    "# avg =     netCDF4.Dataset(resource_directory + 'cccma_h1_average.nc')\n",
    "# stddev =  netCDF4.Dataset(resource_directory + 'cccma_h1_rmssdn.nc')\n",
    "# pavg =    netCDF4.Dataset(resource_directory + 'cccma_h2_average.nc')\n",
    "# pstddev = netCDF4.Dataset(resource_directory + 'cccma_h2_rmssdn.nc')\n",
    "\n",
    "# # MIROC vs CORE\n",
    "# output_filename = 'miroc_core_coreareas.csv'\n",
    "# avg =     netCDF4.Dataset(resource_directory + 'core_average.nc')\n",
    "# stddev =  netCDF4.Dataset(resource_directory + 'core_rmssdn.nc')\n",
    "# pavg =    netCDF4.Dataset(resource_directory + 'miroc_average.nc')\n",
    "# pstddev = netCDF4.Dataset(resource_directory + 'miroc_rmssdn.nc')\n",
    "\n",
    "# # First half vs second half of MIROC\n",
    "# output_filename = 'miroc_h1h2_coreareas.csv'\n",
    "# avg =     netCDF4.Dataset(resource_directory + 'miroc_h1_average.nc')\n",
    "# stddev =  netCDF4.Dataset(resource_directory + 'miroc_h1_rmssdn.nc')\n",
    "# pavg =    netCDF4.Dataset(resource_directory + 'miroc_h2_average.nc')\n",
    "# pstddev = netCDF4.Dataset(resource_directory + 'miroc_h2_rmssdn.nc')\n",
    "\n",
    "# # ECHOG vs CORE\n",
    "# output_filename = 'echog_core_coreareas.csv'\n",
    "# avg =     netCDF4.Dataset(resource_directory + 'core_average.nc')\n",
    "# stddev =  netCDF4.Dataset(resource_directory + 'core_rmssdn.nc')\n",
    "# pavg =    netCDF4.Dataset(resource_directory + 'echog_average.nc')\n",
    "# pstddev = netCDF4.Dataset(resource_directory + 'echog_rmssdn.nc')\n",
    "\n",
    "# # First half vs second half of ECHOG\n",
    "# output_filename = 'echog_h1h2_coreareas.csv'\n",
    "# avg =     netCDF4.Dataset(resource_directory + 'echog_h1_average.nc')\n",
    "# stddev =  netCDF4.Dataset(resource_directory + 'echog_h1_rmssdn.nc')\n",
    "# pavg =    netCDF4.Dataset(resource_directory + 'echog_h2_average.nc')\n",
    "# pstddev = netCDF4.Dataset(resource_directory + 'echog_h2_rmssdn.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load latitude and longitude arrays\n",
    "latitude = np.array(avg.variables['LATITUDE'])\n",
    "longitude = np.array(avg.variables['LONGITUDE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up depth bins, for these data, they are in meters\n",
    "depthbins = pd.DataFrame({'mindepth': [0, 75],\n",
    "                          'maxdepth': [60, 200]},\n",
    "                         index=['shallow','deep'],\n",
    "                         columns=['mindepth', 'maxdepth', 'minz', 'maxz'])\n",
    "\n",
    "depth = np.array(avg.variables['zsalt'])\n",
    "depthindices=list()\n",
    "for d in range(len(depthbins.index)):\n",
    "    indicesz=np.where(np.logical_and(depth[:] <= depthbins.maxdepth[d],\n",
    "                                     depth[:] >= depthbins.mindepth[d]))\n",
    "    depthindices.append(indicesz[0])\n",
    "    if len(depthindices[d]) > 0:\n",
    "        depthbins.minz[d] = min(depthindices[d])\n",
    "        depthbins.maxz[d] = max(depthindices[d])\n",
    "        \n",
    "depthbins['label']=['0 to 60 m', '75 to 200 m']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But each of these also need four slots for the [avg, stddev, pavg, pstddev]. I think it's best to create a new dataframe for each variable, test for depth, and if it has depth create the bins and add the columns to the individual dataframe. At the end, we can add all the dataframes together for output via a CSV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This is our known metadata for the outputs\n",
    "metadata = pd.DataFrame({\n",
    "         'orgName': [\n",
    "                   #these don't have depth\n",
    "                   'icephl_latlon','ben_latlon',\n",
    "                   'aice_latlon',\n",
    "                   #these do\n",
    "                 'phs_latlon','phl_latlon','mzl_latlon','cop_latlon','ncao_latlon','ncas_latlon','eup_latlon','det_latlon',\n",
    "                 'temp_latlon',\n",
    "                 'u_latlon',\n",
    "                 'v_latlon',],\n",
    "         'name': ['Ice Phytoplankton Concentration',\n",
    "                  'Benthos Concentration',\n",
    "                  'Sea Ice Area Fraction',\n",
    "                  'Small Phytoplankton Concentration',\n",
    "                  'Large Phytoplankton Concentration',\n",
    "                  'Large Microzooplankton Concentration',\n",
    "                  'Small Coastal Copepod Concentration',\n",
    "                  'Offshore Neocalanus Concentration',\n",
    "                  'Neocalanus Concentration',\n",
    "                  'Euphausiids Concentration',\n",
    "                  'Detritus Concentration',\n",
    "                  'Sea Water Temperature',\n",
    "                  'Zonal (U) Current',\n",
    "                  'Meridional (V) Current'],\n",
    "         'units': ['mgC/m2','mgC/m2',\n",
    "                   'Fraction',\n",
    "                   'mgC/m3','mgC/m3','mgC/m3','mgC/m3','mgC/m3','mgC/m3','mgC/m3','mgC/m3',\n",
    "                   'degrees C',\n",
    "                   'm/s',\n",
    "                   'm/s']\n",
    "       })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some shapes will be empty (i.e., there's a polygon, but no model data), others will have very few values. We should keep track of how many model pixels are going into each resulting mean. For the combined IBAs, there are 210 polygons, identified by sitenums, objectids, and their geometries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Store the results by keeping track of the data in a Panda Dataframes\n",
    "We will append columns to these and fill in the data as we get it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create a dataframe to house the IBA site information, indexed by the IBA sitenums.\n",
    "siteinfo = pd.DataFrame(\n",
    "                        {\n",
    "                        'sitename': sitenames,\n",
    "                        'iba_type': iba_types,\n",
    "                        'profile': profiles,\n",
    "                        'species': species,\n",
    "                        'minlat': minlat,\n",
    "                        'minlon': minlon,\n",
    "                        'maxlat': maxlat,\n",
    "                        'maxlon': maxlon,\n",
    "                        'tested': (['no'] * nsites)\n",
    "                        }\n",
    "                       )\n",
    "\n",
    "#We'll also add one more row, for the whole area, as a reference\n",
    "areainfo = pd.DataFrame(\n",
    "                        {\n",
    "                        'minlat': min(latitude),\n",
    "                        'minlon': min(longitude),\n",
    "                        'maxlat': max(latitude),\n",
    "                        'maxlon': max(longitude),\n",
    "                        'miny': [0],\n",
    "                        'minx': [0],\n",
    "                        'maxy': [len(latitude)],\n",
    "                        'maxx': [len(longitude)],\n",
    "                        'tested': ['yes']\n",
    "                        },\n",
    "                        index = ['Whole Area']\n",
    "                       )\n",
    "\n",
    "siteinfo=siteinfo.append(areainfo)\n",
    "siteinfo=siteinfo[['sitename', 'iba_type', 'profile','species', 'minlat', 'maxlat', 'minlon', 'maxlon', 'miny', 'maxy', 'minx', 'maxx', 'tested']]\n",
    "nsites=nsites+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now convert the latlons to pixel indices for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "latindices = list()\n",
    "lonindices = list()\n",
    "\n",
    "for x in range(nsites):\n",
    "    #first do latitude\n",
    "    indicesy = np.where(np.logical_and(latitude <= siteinfo.maxlat[x],\n",
    "                                       latitude >= siteinfo.minlat[x]))\n",
    "    latindices.append(indicesy[0])\n",
    "    #then longitude\n",
    "    #PMEL models are in positive east, so we have to convert our bounding box.\n",
    "    indicesx = np.where(np.logical_and(longitude[:] <= siteinfo.maxlon[x],\n",
    "                                       longitude[:] >= siteinfo.minlon[x]))\n",
    "    lonindices.append(indicesx[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#finds the minimum and maxium pixel indices for each sitenum\n",
    "avgdata = ma.masked_array(avg.variables[metadata.orgName[0]][:])\n",
    "for x in range(nsites-1):\n",
    "    if len(latindices[x]) > 0:\n",
    "        siteinfo.miny[x] = min(latindices[x])\n",
    "        siteinfo.maxy[x] = max(latindices[x])\n",
    "    if len(lonindices[x]) > 0:\n",
    "        siteinfo.minx[x] = min(lonindices[x])\n",
    "        siteinfo.maxx[x] = max(lonindices[x]) \n",
    "#This tests to see if there are any NaNs, meaning the IBA goes off the edge of the model.\n",
    "#We'll skip any of those IBAs by setting a \"Tested\" variable to 0.\n",
    "    if pd.notnull(siteinfo.miny[x]):\n",
    "        if pd.notnull(siteinfo.maxy[x]):\n",
    "            if pd.notnull(siteinfo.minx[x]):\n",
    "                if pd.notnull(siteinfo.maxx[x]):\n",
    "                    #This will find if there are any non-masked values to test.\n",
    "                    count = ma.count(avgdata[0, siteinfo.miny[x]:siteinfo.maxy[x]+1, siteinfo.minx[x]:siteinfo.maxx[x]+1])\n",
    "                    if count > 0:\n",
    "                        siteinfo.tested[x] = 'yes'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Process the data into a list of dataframes, indexed by sitenum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the pixel mins and maxes, we can iterate through the variables:\n",
    "\n",
    "1. Check for depth: if present iterate three times for depth bins, if absent, just do once.  \n",
    "2. Using the pixel indexes to subset each model variable for avg, stddev, pavg, and pstddev arrays and average those values.  \n",
    "3. Create a data frame with the mean avg, stddev, pavg, and pstddev for each sitenum, and append to list of dataframe results\n",
    "4. Create a label including the variable name and depth bin, append to another list of results. There should be one label for each dataframe.\n",
    "5. We'll concatenate these dataframes to the siteinfo dataframe into large dataframe for output to CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nvariables = len(metadata.orgName)\n",
    "#create an empty dataframe\n",
    "dfresults = pd.DataFrame(index = siteinfo.index)\n",
    "\n",
    "#for each variable\n",
    "for i in range(nvariables): \n",
    "    #Load in the data\n",
    "    avgdata = ma.masked_array(avg.variables[metadata.orgName[i]][:])\n",
    "    mask=ma.getmask(avgdata)\n",
    "    stddevdata = ma.masked_array(stddev.variables[metadata.orgName[i]][:], mask)\n",
    "    pavgdata = ma.masked_array(pavg.variables[metadata.orgName[i]][:], mask)\n",
    "    pstddevdata = ma.masked_array(pstddev.variables[metadata.orgName[i]][:], mask)\n",
    "    \n",
    "    #check for depth\n",
    "    if len(avgdata.shape) == 3:\n",
    "        avgvals = list()\n",
    "        stddevvals = list()\n",
    "        pavgvals = list()\n",
    "        pstddevvals = list()\n",
    "        count = list()\n",
    "        \n",
    "        for x in range(nsites):\n",
    "            if (siteinfo.tested[x] == 'yes'):\n",
    "                #mean of data(latitude_indices, longitude_indices)\n",
    "                #The +1 is necessary because numpy indexing goes from start:end, but end is the first value that's NOT included.\n",
    "                #Whereas the way I've set this up, the maxy and maxx SHOULD be included.\n",
    "                avgvals.append(ma.mean(avgdata[0, siteinfo.miny[x]:siteinfo.maxy[x]+1, siteinfo.minx[x]:siteinfo.maxx[x]+1]))\n",
    "                stddevvals.append(ma.mean(stddevdata[0, siteinfo.miny[x]:siteinfo.maxy[x]+1, siteinfo.minx[x]:siteinfo.maxx[x]+1]))\n",
    "                pavgvals.append(ma.mean(pavgdata[0, siteinfo.miny[x]:siteinfo.maxy[x]+1, siteinfo.minx[x]:siteinfo.maxx[x]+1]))\n",
    "                pstddevvals.append(ma.mean(pstddevdata[0, siteinfo.miny[x]:siteinfo.maxy[x]+1, siteinfo.minx[x]:siteinfo.maxx[x]+1]))\n",
    "                count.append(ma.count(avgdata[0, siteinfo.miny[x]:siteinfo.maxy[x]+1, siteinfo.minx[x]:siteinfo.maxx[x]+1]))\n",
    "\n",
    "            else:\n",
    "                avgvals.append(np.nan)\n",
    "                stddevvals.append(np.nan)\n",
    "                pavgvals.append(np.nan)\n",
    "                pstddevvals.append(np.nan)\n",
    "                count.append(np.nan)\n",
    "        \n",
    "        dfresults[metadata.name[i]+' Count'] = count\n",
    "        dfresults[metadata.name[i]+' Avg'] = avgvals\n",
    "        dfresults[metadata.name[i]+' StdDev'] = stddevvals\n",
    "        dfresults[metadata.name[i]+' PAvg'] = pavgvals\n",
    "        dfresults[metadata.name[i]+' PStdDEv'] = pstddevvals\n",
    "\n",
    "\n",
    "    if len(avgdata.shape) == 4:\n",
    "        for d in range(len(depthbins.index)):\n",
    "            avgvals = list()\n",
    "            stddevvals = list()\n",
    "            pavgvals = list()\n",
    "            pstddevvals = list()\n",
    "            count = list()\n",
    "            \n",
    "            for x in range(nsites):\n",
    "                \n",
    "                if (siteinfo.tested[x] == 'yes'):\n",
    "                    #mean of data(latitude_indices, longitude_indices)\n",
    "                    #The +1 is necessary because numpy indexing goes from start:end, but end is the first value that's NOT included.\n",
    "                    #Whereas the way I've set this up, the maxy and maxx SHOULD be included.\n",
    "                    count.append(ma.count(avgdata[0, depthbins.minz[d]:depthbins.maxz[d]+1,\n",
    "                              siteinfo.miny[x]:siteinfo.maxy[x]+1,\n",
    "                              siteinfo.minx[x]:siteinfo.maxx[x]+1]))\n",
    "                    if (count[x] > 0):\n",
    "                        avgvals.append(ma.mean(avgdata[0, depthbins.minz[d]:depthbins.maxz[d]+1,\n",
    "                                                       siteinfo.miny[x]:siteinfo.maxy[x]+1,\n",
    "                                                       siteinfo.minx[x]:siteinfo.maxx[x]+1]))\n",
    "                        stddevvals.append(ma.mean(stddevdata[0, depthbins.minz[d]:depthbins.maxz[d]+1,\n",
    "                                                             siteinfo.miny[x]:siteinfo.maxy[x]+1,\n",
    "                                                             siteinfo.minx[x]:siteinfo.maxx[x]+1]))\n",
    "                        pavgvals.append(ma.mean(pavgdata[0, depthbins.minz[d]:depthbins.maxz[d]+1,\n",
    "                                                         siteinfo.miny[x]:siteinfo.maxy[x]+1,\n",
    "                                                         siteinfo.minx[x]:siteinfo.maxx[x]+1]))\n",
    "                        pstddevvals.append(ma.mean(pstddevdata[0, depthbins.minz[d]:depthbins.maxz[d]+1,\n",
    "                                                               siteinfo.miny[x]:siteinfo.maxy[x]+1,\n",
    "                                                               siteinfo.minx[x]:siteinfo.maxx[x]+1]))\n",
    "                    else:\n",
    "                        avgvals.append(np.nan)\n",
    "                        stddevvals.append(np.nan)\n",
    "                        pavgvals.append(np.nan)\n",
    "                        pstddevvals.append(np.nan)\n",
    "                else:\n",
    "                    avgvals.append(np.nan)\n",
    "                    stddevvals.append(np.nan)\n",
    "                    pavgvals.append(np.nan)\n",
    "                    pstddevvals.append(np.nan)\n",
    "                    count.append(np.nan)\n",
    "                    \n",
    "            dfresults[metadata.name[i]+' '+depthbins.label[d]+' Count'] = count\n",
    "            dfresults[metadata.name[i]+' '+depthbins.label[d]+' Avg' + ' in ' + metadata.units[i]] = avgvals\n",
    "            dfresults[metadata.name[i]+' '+depthbins.label[d]+' StdDev' + ' in ' + metadata.units[i]] = stddevvals\n",
    "            dfresults[metadata.name[i]+' '+depthbins.label[d]+' PAvg' + ' in ' + metadata.units[i]] = pavgvals\n",
    "            dfresults[metadata.name[i]+' '+depthbins.label[d]+' PStdDEv' + ' in ' + metadata.units[i]] = pstddevvals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to CSV\n",
    "Combine the siteinfo and dfresults dataframes, and write out to CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged = siteinfo.join(dfresults)\n",
    "merged.to_csv(output_filename,index_label='localindex')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
