# -*- coding: utf-8 -*-
# <nbformat>3.0</nbformat>

# <markdowncell>

# #Bering Seabirds and Future Climate Variability
# Assessing the projected change in climate variability of areas used by seabirds in the Bering Sea.

# <codecell>

from utilities import css_styles
css_styles()

# <codecell>

import time
import numpy as np

# <markdowncell>

# ####Load Important Bird Areas
# These are defined by [Audobon](http://web4.audubon.org/bird/iba/) and hosted by AOOS on a geoserver. Currently, they represent areas important for numerous species of birds, but we have the single-species polygons and are working on ingesting them so they can be accessed in the same way.

# <codecell>

from owslib.wfs import WebFeatureService
known_wfs = "http://solo.axiomalaska.com/geoserver/audubon/ows"
wfs = WebFeatureService(known_wfs, version='1.0.0')
geojson_response = wfs.getfeature(typename=['audubon:audubon_ibas'], outputFormat="application/json", srsname="urn:x-ogc:def:crs:EPSG:4326").read()

# <codecell>

import json
from utilities import find_dict_keys

from shapely.geometry import shape, MultiPolygon

geojson = json.loads(geojson_response)    
geometries = find_dict_keys("geometry", geojson)
shapes = [shape(g) for g in geometries]

# <markdowncell>

# A utility to plot the imported IBAs as a reality check. (I'm curently ignoring the issue with the international date line, as it is only a display issue.)
# <div class="error"><strong>Some Polygons Currently Missing</strong> - Comparing the polygons shown below to the [AOOS visualization](http://portal.aoos.org/?v=rand&portal_id=25#module-metadata/8c6e3c7c-4294-11e2-bc24-00219bfe5678/cd96fa58-aeb5-11e2-8a9a-00219bfe5678) shows some differences: (1) Some polygons seem to be missing (e.g., along the Arctic coast), (2) The dot-like polygons along the northern coasts appear to be incomplete polylines. But the bounds for these polygons look correct, so it may just be a strange shapely or folium display issue.</div>

# <codecell>

import folium
from utilities import get_coords

mapper = folium.Map(location=[65.1, -155.6], zoom_start=4)
for s in shapes:
    for c in get_coords(s):
        mapper.line(c, line_color='#FF0000', line_weight=3)
        
mapper.lat_lng_popover()
mapper._build_map()

from IPython.core.display import HTML
HTML('<iframe srcdoc="{srcdoc}" style="width: 100%; height: 535px; border: none"></iframe>'.format(srcdoc=mapper.HTML.replace('"', '&quot;')))

# <markdowncell>

# ### Set a Spatial Filter
# For now, this just uses a bounding box for each IBA rather than polygons. In the future we'll use the actual polygons. (It turns out that using polygon masks in the python numpy and panda libraries is not exactly straightfoward. So we will have to build a function to do that.)

# <codecell>

from shapely.geometry import box

for s in shapes:
# miny, minx, maxy, maxx
    analysis_box = s.bounds
    bound = box(*analysis_box).boundary.coords
    mapper.line(bound, line_color='#0000FF', line_weight=5)
mapper._build_map()

# <codecell>

from IPython.core.display import HTML
HTML('<iframe srcdoc="{srcdoc}" style="width: 100%; height: 535px; border: 1px"></iframe>'.format(srcdoc=mapper.HTML.replace('"', '&quot;')))

# <markdowncell>

# <div class='success'><strong>shapes.bounds</strong> is the multipolygon array that we want to carry forward.</div>

# <markdowncell>

# ### Load model data
# In this (simple) example I'm using the [PMEL CCCma](http://portal.aoos.org/alaska-statewide.php#module-metadata/4f706756-7d57-11e3-bce5-00219bfe5678/a7b34348-8613-4006-99d8-37fd49812aaa) weekly forecast (one time slice every seven days), and querying sea surface temperature to start. We're going to put a bunch of these together though. I'm finding the model URL using the [Axiom THREDDS catalog](http://thredds.axiomalaska.com/thredds/catalogs/aoos.html) of AOOS datasets and OPENDAP protocols. These models are not tiny, so it can take up to 30 seconds to access them.

# <markdowncell>

# ####Set Climate basline model from which to pull data
# [PMEL Co-ordinated Ocean-Ice Reference Experiments (CORE) Climate Model](http://portal.aoos.org/?v=rand&portal_id=25#module-metadata/5626a0b6-7d79-11e3-ac17-00219bfe5678/114f03ec-1c8f-453c-a219-5f9780d97cc3) - 01/19/1969 - 01/02/2005  
# - we have two other models which in theory have data over our baseline: [CFS-R](http://portal.aoos.org/?v=rand&portal_id=25#module-metadata/f8cb79f6-7d59-11e3-a6ee-00219bfe5678/af7b2758-e7d0-4d82-96eb-1696c464586d) and [CFSv2](http://portal.aoos.org/?v=rand&portal_id=25#module-metadata/4e77fed6-7d7a-11e3-97d7-00219bfe5678/0e50760b-8795-4aac-a155-dd624aff9c24). Unfortunately we don't have these data immediately available.
# 
# ####Set Climate projections(s) from which to pull data
# [PMEL MIROC](http://portal.aoos.org/alaska-statewide.php#module-metadata/68ea728a-7d7a-11e3-823b-00219bfe5678/50242dc9-b639-42c4-8287-d0aeb082fb1f) - 12/29/2002 - 12/04/2039  
# [PMEL ECHO-G](http://portal.aoos.org/alaska-statewide.php#module-metadata/18ffa59c-7d7a-11e3-82a4-00219bfe5678/7bfa82c2-3051-484e-8c7b-9b9f711083f5) - 12/29/2002 - 12/04/2039  
# [PMEL CCCma](http://portal.aoos.org/alaska-statewide.php#module-metadata/4f706756-7d57-11e3-bce5-00219bfe5678/6b2d1f96-4c5b-4b13-ad57-c8ca2e326a34) - 01/26/2003 - 01/01/2040

# <markdowncell>

# <div class="warning"><strong>CCCma Model Dates</strong> - CCCma models have slightly different dates. This is because the modeling groups started at slightly different times. Nick Bond says these dates should not be rounded (in order to make them all match up) as it will increase model error.</div>

# <markdowncell>

# By pulling the models through THREDDS, we were leveraging the Java netCDF library which takes advantage of ncml virtual aggregated datasets (lots of files, but just looks like one).Â By using local data, we lose that ability and will have to aggregate manually.

# <codecell>

# Global sea surface temperature - just as a test
#model = 'http://thredds.axiomalaska.com/thredds/dodsC/G1_SST.nc'
#standard_name = 'sea_surface_temperature'

            
#PMEL Forecast Models:
models = [
        '/augie/gluster/data/netCDF/pmel/core/out.nc'
        '/augie/gluster/data/netCDF/pmel/core/core_week1001.nc', #CORE hindcast
        'http://thredds.axiomalaska.com/thredds/dodsC/PMEL_MIROC.nc', #MIROC forecast
        'http://thredds.axiomalaska.com/thredds/dodsC/PMEL_ECHOG.nc', #ECHO-G forecast
        'http://thredds.axiomalaska.com/thredds/dodsC/PMEL_CCCMA1.nc', #CCCma forecast
        '/augie/gluster/data/netCDF/pmel/cccma/cccma_week1002.nc'
        ]

model_names = [
              'CORE hindcast',
              'MIROC forecast',
              'ECHO-G forecast',
              'CCCma forecast',
              'CCCMA forecast'
              ]

#Pre-loop phase
model_index=0

import netCDF4
nc = netCDF4.Dataset(models[model_index])
#forecast_nc = netCDF4.Dataset(forecast[forecast_index])

# <markdowncell>

# #### Set variable(s) from which to pull data
# Two of the variables just have surface values. Twelve others have depth as well. We need a combination.

# <codecell>

#Variables with time and depth coordinates
#biological
variable_index = 10

original_names=['icephl_latlon',
                'phs_latlon',
                'phl_latlon',
                'mzl_latlon',
                'cop_latlon',
                'ncao_latlon',
                'ncas_latlon',
                'eup_latlon',
                'det_latlon',
                'ben_latlon',
                'temp_latlon',
                'aice_latlon',
                'u_latlon',
                'v_latlon'
                ]

standard_names=[
                'ice_phytoplankton_concentration', #(no depth)
                'small_phytoplankton_concentration',
                'large_phytoplankton_concentration',
                'large_microzooplankton_concentration',
                'small_coastal_copepod_concentration',
                'offshore_neocalanus_concentration',
                'neocalanus_concentration',
                'euphausiids_concentration',
                'detritus_concentration',
                'benthos_concentration', #benthic infauna
                'sea_water_temperature',
                'salinity',
                'sea_ice_area_fraction',
                'u_current', #Zonal Current 
                'v_current' #Meridional Current
                ]
standard_names[variable_index]

# <codecell>

variable = nc.variables[original_names[variable_index]]

# <codecell>

#from utilities import get_variable_from_standard
#variable = get_variable_from_standard(nc, standard_names[variable_index])[0]

print variable.dimensions
print variable.shape

# <markdowncell>

# <div class="warning"><strong>Lots of Data</strong> - So it turns out that this is a lot of data (1.2 billion points), and it can't all be loaded into memory at once. So instead we need to iterate through calculations.</div>

# <codecell>

figure(num=None, figsize=(16,10))
title(model_names[model_index] + ', '+ standard_names[variable_index]
      )
img = plt.imshow(variable[0,0],
           interpolation='nearest',
           origin='lower',
           cmap='jet')
colorbar(img, orientation='vertical', shrink=0.8)
plt.show()

# <markdowncell>

# ## Calculate Mapped Mean and Standard Deviations for baseline period
# We need to map the spatial variation of Bering-wide standard deviations away from Bering-wide mean.
# 
# 1. Calculate the latlon-dependent mean for each depth bin.
# 2. Calculate Bering-wide mean of variable across space, time and depth
# 3. Calculate Bering-wide standard deviation across space and time for each depth bin
# 4. Calculate ABS((latlon-dependent mean - Bering-wide mean)/(Bering-wide standard deviation))

# <markdowncell>

# ### Set a time filter
# This is the time that we will search over in the model data. We'll eventually use this aggregate 

# <codecell>

#Base Time Frame
base_starttime = '1970-01-01'
base_stoptime = '2000-12-31'

#Projected Time Frame
proj_starttime = '2010-01-01'  # UTC
proj_stoptime = '2010-12-01' # short time frame, just to test the system
#proj_stoptime = "2039-12-01"  # UTC - can't be 2040 because of the CCCma model

import pytz
from dateutil.parser import parse
min_time = parse(base_starttime).replace(tzinfo=pytz.utc)
max_time = parse(base_stoptime).replace(tzinfo=pytz.utc)

# <markdowncell>

# The following cell tries to match up the time filter (in UTC) to whatever the time units are in the model data (in this case listed in seconds).

# <codecell>

nc.variables['TIME'].standard_name

# <codecell>

import calendar
from datetime import datetime

time_var = nc.variables['TIME']
time_units = "seconds since 1900-01-01 00:00:00"
#time_var = get_variable_from_standard(nc, 'TIME')[0]

#The first part of this IF statement was because our test model (G1SST) had a bad time conversion.
#It may no longer be necessary (but keep the ELSE).
if time_var.dtype == np.dtype('S1'):
    def str_to_epoch(date_str):
        return calendar.timegm(parse(date_str).replace(tzinfo=pytz.utc).timetuple())
    new_time = netCDF4.chartostring(time_var[:])
    
    #time_var.units = "seconds since 1970-01-01"
    new_min_time = calendar.timegm(min_time.timetuple())
    new_max_time = calendar.timegm(max_time.timetuple())
    np_str_to_epoch = np.vectorize(str_to_epoch)
    new_time = np_str_to_epoch(new_time)
    time_indices = np.logical_and(new_time[:] <= new_max_time,
                                  new_time[:] >= new_min_time)
    time_indices = np.where(time_indices)
    time_data = new_time[time_indices]
    time_data = netCDF4.num2date(time_data, time_var.units)
else:
    new_min_time, new_max_time = netCDF4.date2num([min_time, max_time], time_units)
    time_indices = np.logical_and(time_var[:] <= new_max_time,
                                  time_var[:] >= new_min_time)
    time_indices = np.where(time_indices)
    time_data = time_var[time_indices]
#    time_data = netCDF4.num2date(time_data, time_units)

# <codecell>

print time_var.size

# <codecell>

print time_data.size

# <markdowncell>

# ### Set a Depth Filter
# We want a series of bins that go from 0-10; 10-60; and 60-200.

# <codecell>

minz = 0
maxz = 10

depth_var = get_variable_from_standard(nc, "depth")[0]
depth_indices = np.logical_and(depth_var[:] <= maxz,
                             depth_var[:] >= minz)
depth_indices = np.where(depth_indices)
depth_data = depth_var[depth_indices]

# <codecell>

print depth_data.size

# <markdowncell>

# ### Subset the Model Variable
# Print our previously found time, depth, latitude and longitude indices.

# <markdowncell>

# <div class='warning' ><strong>Memory errors</strong> - Running into some memory issues here because of the size of the array being called (the nc file with the models is 112018.0 Mbytes, or 112 Gb). In addition, the netCDF4 library doesn't like forking over this much data at once and gives a "Malformed or inacessible DAP DATADDS" error if the array has too many elements.</div>

# <markdowncell>

# This memory issue is impassible. Because we can't load in all the data at once, we need to do this via a loop. The [solution](http://stackoverflow.com/questions/10365119/mean-value-and-standard-deviation-of-a-very-huge-data-set) based on [rearrangement of the StdDeviation equation](http://www.derivations.org/stdev.pdf) is to only keep track of three numbers in actual memory:
# 
# 1. The sum of all values (sum_x)
# 2. The sum of their squares (sum_x2)
# 3. Total count of all values (n)
# 
# Mean = sum_x/n  
# Standard Deviation = (sum_x2 - (sum_x)<sup>2</sup>/n)/(n-1)

# <codecell>

print time_indices[0]
time_indices[0]
print depth_indices[0]
print variable.shape
print variable

# <markdowncell>

# Note: we have to be careful here because variable is a masked array. We only want to add to n the values that are unmasked (i.e., have data). Here I'm using the compressed() method, which loses the spatial component but returns just unmasked values .

# <markdowncell>

# <div class='warning'><strong>Takes a long time</strong> - Pulling 1 year of data for 3 depths for a single variable takes about 20 minutes. Estimating forward, 30 years of data will take around 10 hours.

# <codecell>

starttime = time.time()
sum_x = np.float64(0.)
sum_x2 = np.float64(0.)
n = np.int64(0)

print 'Start Time: ' + str(min_time)
print 'Stop Time: ' + str(max_time)

for t in time_indices[0]:
    for d in depth_indices[0]: # this is safety, in case the depth bin is very large
        flatslice=variable[t, d,:,:].compressed() # returns a 1D array of non-masked values
        sum_x = sum_x + np.sum(flatslice)
        sum_x2 = sum_x2 + np.sum(np.square(flatslice))
        n = n + flatslice.size
    print time_data[t-time_indices[0][0]]
    
marktime = time.time()-starttime
print 'Operation took ' + str(round(marktime/3600, 2)) + ' hours.'

# <codecell>

bw_mean = sum_x/n
bw_stddev = np.sqrt((sum_x2 - np.square(sum_x)/n)/(n-1))

print 'Bering-wide Mean:', bw_mean
print 'Bering-wide StdDev: ', bw_stddev

# <markdowncell>

# The following do mean and standard deviations over unique subsets too, but they generally suck up too much memory, and can't be called with too many time or depth indices. So I've commented them out for now.

# <rawcell>

# #subset = variable[time_indices[0], depth_indices[0], *, *]
# subset = variable[time_indices[0],depth_indices[0],:,:]
# #subset_variable = np.squeeze(subset_variable)
# print subset.shape

# <rawcell>

# starttime = time.time()
# mean = np.mean(variable, axes=(1,2))
# marktime = time.time()-starttime
# print str(marktime) + ' seconds'
# 
# stddev = np.std(variable, axes=(1,2))
# stoptime = time.time()-marktime
# print str(marktime) + ' seconds'

# <markdowncell>

# #### Subset model data by our bounding box
# Which will soon be a grouping of polygons.

# <codecell>

import numpy as np

lat_var = get_variable_from_standard(nc, "latitude")[0]
lat_indices = np.logical_and(lat_var[:] <= maxy,
                             lat_var[:] >= miny)
lat_indices = np.where(lat_indices)
lat_data = lat_var[lat_indices]


lon_var = get_variable_from_standard(nc, "longitude")[0]
#PMEL models are in positive east, so we have to convert our bounding box.
minx_modulo360 = minx + 360.
maxx_modulo360 = maxx + 360.

lon_indices = np.logical_and(lon_var[:] <= maxx_modulo360,
                             lon_var[:] >= minx_modulo360)

lon_indices = np.where(lon_indices)
lon_data = lon_var[lon_indices]

# <codecell>

import pandas as pd
data = pd.Panel(subset_variable, items=time_data, major_axis=lat_data, minor_axis=lon_data)

# <markdowncell>

# ##Plots
# Just a few quick plots to spur discussion.

# <markdowncell>

# ###Mean
# Currently set to be the mean value of the region of interest.

# <codecell>

figure(num=None, figsize=(16,5))
plot(time_data[0:150], means[0:150], color='black', marker='o', markersize=4, markerfacecolor='red', markeredgewidth=1, label='Mean')
ylabel(standard_name)
grid(True)
legend(loc=2, ncol=1, borderaxespad=0.5)
show()

# <codecell>

figure(num=None, figsize=(16,5))
plot(time_data, means, 'k', label='Mean')
xlabel('Year')
ylabel(standard_name)
legend(loc=2, ncol=1, borderaxespad=0.5)
grid(True)
show()

# <markdowncell>

# ###Standard Deviation
# This is currently the standard deviation across each time slice. That's not really what we want though. We want a sense of standard deviation over time.

# <codecell>

figure(num=None, figsize=(16,5))
plot(time_data[0:150], stddevs[0:150], color='blue', marker='o', markersize=4, markerfacecolor='blue', markeredgewidth=1, label='Std Dev')
ylabel(standard_name)
grid(True)
legend(loc=2, ncol=1, borderaxespad=0.5)
show()

# <codecell>

figure(num=None, figsize=(16,5))
plot(time_data, stddevs, 'b', label='Std Dev')
xlabel('Year')
ylabel(standard_name)
legend(loc=2, ncol=1, borderaxespad=0.5)
grid(True)
show()

# <codecell>


