# -*- coding: utf-8 -*-
# <nbformat>3.0</nbformat>

# <markdowncell>

# #Bering Seabirds and Future Climate Variability
# Assessing the projected change in climate variability of areas used by seabirds in the Bering Sea.

# <codecell>

from utilities import css_styles
css_styles()

# <markdowncell>

# ####Load Important Bird Areas
# These are defined by [Audobon](http://web4.audubon.org/bird/iba/) and hosted by AOOS on a geoserver. Currently, they represent areas important for numerous species of birds, but we have the single-species polygons and are working on ingesting them so they can be accessed in the same way.

# <codecell>

from owslib.wfs import WebFeatureService
known_wfs = "http://solo.axiomalaska.com/geoserver/audubon/ows"
wfs = WebFeatureService(known_wfs, version='1.0.0')
geojson_response = wfs.getfeature(typename=['audubon:audubon_ibas'], outputFormat="application/json", srsname="urn:x-ogc:def:crs:EPSG:4326").read()

# <codecell>

import json
from utilities import find_dict_keys

from shapely.geometry import shape, MultiPolygon

geojson = json.loads(geojson_response)    
geometries = find_dict_keys("geometry", geojson)
shapes = [shape(g) for g in geometries]

# <markdowncell>

# A utility to plot the imported IBAs as a reality check. (I'm curently ignoring the issue with the international date line, as it is only a display issue.)

# <codecell>

import folium
from utilities import get_coords

map_center = shapes[0].centroid
mapper = folium.Map(location=[65.1, -155.6], zoom_start=4)
for s in shapes:
    for c in get_coords(s):
        mapper.line(c, line_color='#FF0000', line_weight=5)
mapper.lat_lng_popover()
mapper._build_map()

from IPython.core.display import HTML
HTML('<iframe srcdoc="{srcdoc}" style="width: 100%; height: 535px; border: none"></iframe>'.format(srcdoc=mapper.HTML.replace('"', '&quot;')))

# <markdowncell>

# ### Set a Spatial Filter
# For now, this just uses a manual box for one IBA in the Bering Sea. In the future, I'll try to (1) create boxes for each polygon automagically, (2) use the actual polygons. (It turns out that using polygon masks in the python numpy and panda libraries is not exactly straightfoward. So we will have to build a function to do that.)

# <codecell>

# miny, minx, maxy, maxx
minx = -174
miny = 61.38
maxx = -169.9
maxy = 63.17
analysis_box = miny, minx, maxy, maxx

from shapely.geometry import box
bound = box(*analysis_box).boundary.coords
mapper.line(bound, line_color='#0000FF', line_weight=5)
mapper._build_map()

# <codecell>

from IPython.core.display import HTML
HTML('<iframe srcdoc="{srcdoc}" style="width: 100%; height: 535px; border: none"></iframe>'.format(srcdoc=mapper.HTML.replace('"', '&quot;')))

# <markdowncell>

# ### Load model data
# In this (simple) example I'm using the [PMEL CCCma](http://portal.aoos.org/alaska-statewide.php#module-metadata/4f706756-7d57-11e3-bce5-00219bfe5678/a7b34348-8613-4006-99d8-37fd49812aaa) weekly forecast (one time slice every seven days), and querying sea surface temperature to start. We're going to put a bunch of these together though. I'm finding the model URL using the [Axiom THREDDS catalog](http://thredds.axiomalaska.com/thredds/catalogs/aoos.html) of AOOS datasets and OPENDAP protocols. These models are not tiny, so it can take up to 30 seconds to access them.

# <markdowncell>

# ####Set Climate model(s) from which to pull data
# [PMEL MIROC](http://portal.aoos.org/alaska-statewide.php#module-metadata/68ea728a-7d7a-11e3-823b-00219bfe5678/50242dc9-b639-42c4-8287-d0aeb082fb1f) - 12/29/2002 03:00 - 12/04/2039  
# [PMEL ECHO-G](http://portal.aoos.org/alaska-statewide.php#module-metadata/18ffa59c-7d7a-11e3-82a4-00219bfe5678/7bfa82c2-3051-484e-8c7b-9b9f711083f5) - 12/29/2002 03:00 - 12/04/2039  
# [PMEL CCCma](http://portal.aoos.org/alaska-statewide.php#module-metadata/4f706756-7d57-11e3-bce5-00219bfe5678/6b2d1f96-4c5b-4b13-ad57-c8ca2e326a34) - 01/26/2003 03:00 - 01/01/2040 

# <markdowncell>

# <div class="warning"><strong>CCCma Model Dates</strong> - For some reason the CCCma model has a slightly different date range listed in the netCDF files. Not sure if this is real (i.e., they ran the model over a different time period) or a mistake in their documentation.</div>

# <codecell>

# Global sea surface temperature - just as a test
#model = 'http://thredds.axiomalaska.com/thredds/dodsC/G1_SST.nc'
#standard_name = 'sea_surface_temperature'

#PMEL Models:
#model = 'http://thredds.axiomalaska.com/thredds/dodsC/PMEL_MIROC.nc' #MIROC
#model = 'http://thredds.axiomalaska.com/thredds/dodsC/PMEL_ECHOG.nc' #ECHO-G
model = 'http://thredds.axiomalaska.com/thredds/dodsC/PMEL_CCCMA1.nc' #CCCma

import netCDF4
nc = netCDF4.Dataset(model)

# <markdowncell>

# #### Set variable(s) from which to pull data
# Two of the variables just have surface values. Twelve others have depth as well. Are we only interested in the surface values?

# <codecell>

#Variables with time coordinates
#standard_name = 'ice_phytoplankton_concentration'
#standard_name = 'sea_ice_area_fraction'

#Variables with time and depth coordinates
#standard_name = 'benthos_concentration'
#standard_name = 'detritus_concentration'
#standard_name = 'euphausiids_concentration'
#standard_name = 'large_microzooplankton_concentration'
#standard_name = 'large_phytoplankton_concentration'
#standard_name = 'neocalanus_concentration'
#standard_name = 'offshore_neocalanus_concentration'
standard_name = 'sea_water_temperature'
#standard_name = 'small_coastal_copepod_concentration'
#standard_name = 'small_phytoplankton_concentration'
#standard_name = 'u_current' #Zonal Current 
#standard_name = 'v_current' #Meridional Current

from utilities import get_variable_from_standard
variable = get_variable_from_standard(nc, standard_name)[0]

print variable.shape
print variable.dimensions

# <markdowncell>

# #### Subset model data by our bounding box
# Which will soon be a grouping of polygons. Soooooon.

# <codecell>

import numpy as np

lat_var = get_variable_from_standard(nc, "latitude")[0]
lat_indexes = np.logical_and(lat_var[:] <= maxy,
                             lat_var[:] >= miny)
lat_indexes = np.where(lat_indexes)
lat_data = lat_var[lat_indexes]


lon_var = get_variable_from_standard(nc, "longitude")[0]
#PMEL models are in positive east, so we have to convert our bounding box.
minx_modulo360 = minx + 360.
maxx_modulo360 = maxx + 360.

lon_indexes = np.logical_and(lon_var[:] <= maxx_modulo360,
                             lon_var[:] >= minx_modulo360)

lon_indexes = np.where(lon_indexes)
lon_data = lon_var[lon_indexes]

# <markdowncell>

# ### Set a time filter
# This is the time that we will search over in the model data. We'll eventually use this aggregate 

# <codecell>

min_time = "2010-01-01"  # UTC
max_time = "2030-01-01"  # UTC

import pytz
from dateutil.parser import parse
min_time = parse(min_time).replace(tzinfo=pytz.utc)
max_time = parse(max_time).replace(tzinfo=pytz.utc)

# <markdowncell>

# The following cell tries to match up the time filter (in UTC) to whatever the time units are in the model data (in this case listed in seconds).

# <codecell>

import calendar
from datetime import datetime

time_var = get_variable_from_standard(nc, "time")[0]

#The first part of this IF statement was because our test model (G1SST) had a bad time conversion.
#It may no longer be necessary (but keep the ELSE).
if time_var.dtype == np.dtype('S1'):
    def str_to_epoch(date_str):
        return calendar.timegm(parse(date_str).replace(tzinfo=pytz.utc).timetuple())
    new_time = netCDF4.chartostring(time_var[:])
    
    #time_var.units = "seconds since 1970-01-01"
    new_min_time = calendar.timegm(min_time.timetuple())
    new_max_time = calendar.timegm(max_time.timetuple())
    np_str_to_epoch = np.vectorize(str_to_epoch)
    new_time = np_str_to_epoch(new_time)
    time_indexes = np.logical_and(new_time[:] <= new_max_time,
                                  new_time[:] >= new_min_time)
    time_indexes = np.where(time_indexes)
    time_data = new_time[time_indexes]
    time_data = netCDF4.num2date(time_data, time_var.units)
else:
#    time_var.units = "seconds since 1900-01-01 00:00:00"
    new_min_time, new_max_time = netCDF4.date2num([min_time, max_time], time_var.units)
    time_indexes = np.logical_and(time_var[:] <= new_max_time,
                                  time_var[:] >= new_min_time)
    time_indexes = np.where(time_indexes)
    time_data = time_var[time_indexes]
    time_data = netCDF4.num2date(time_data, time_var.units)

# <markdowncell>

# ### Set a Depth Filter
# I think we just want the surface, so I will just set it to be 0.

# <codecell>

minz = 0
maxz = 0

depth_var = get_variable_from_standard(nc, "depth")[0]

depth_indexes = np.logical_and(depth_var[:] <= maxz,
                             depth_var[:] >= minz)
depth_indexes = np.where(depth_indexes)
depth_data = depth_var[depth_indexes]

# <markdowncell>

# ### Subset the Model Variable
# Using our previously found time, latitude and longitude indices. In this step, I'm just using the surface (zsalt = 0)

# <codecell>

subset_variable = variable[time_indexes[0], depth_indexes[0], lat_indexes[0], lon_indexes[0]]
subset_variable = np.squeeze(subset_variable)
print subset_variable.shape

# <codecell>

import pandas as pd
data = pd.Panel(subset_variable, items=time_data, major_axis=lat_data, minor_axis=lon_data)

# <markdowncell>

# ###Calculate Things

# <codecell>

means = np.mean(data.values, axis=(1,2))
stddevs = np.std(data.values, axis=(1,2))

# <markdowncell>

# ##Plots
# Just a few quick plots to spur discussion.

# <codecell>

import matplotlib.pyplot as plt

# <markdowncell>

# ###Mean
# Currently set to be the mean value of the region of interest.

# <codecell>

figure(num=None, figsize=(16,5))
plot(time_data[0:150], means[0:150], color='black', marker='o', markersize=4, markerfacecolor='red', markeredgewidth=1, label='Mean')
ylabel(standard_name)
grid(True)
legend(loc=2, ncol=1, borderaxespad=0.5)
show()

# <codecell>

figure(num=None, figsize=(16,5))
plot(time_data, means, 'k', label='Mean')
plot(time_data, stddevs, 'b', label='Std Dev')
xlabel('Year')
ylabel(standard_name)
legend(loc=2, ncol=1, borderaxespad=0.5)
grid(True)
show()

# <markdowncell>

# ###Standard Deviation
# This is currently the standard deviation across each time slice. That's not really what we want though. We want a sense of standard deviation over time.

# <codecell>

figure(num=None, figsize=(16,5))
plot(time_data[0:150], stddevs[0:150], color='blue', marker='o', markersize=4, markerfacecolor='blue', markeredgewidth=1, label='Std Dev')
ylabel(standard_name)
grid(True)
legend(loc=2, ncol=1, borderaxespad=0.5)
show()

# <codecell>

figure(num=None, figsize=(16,5))
plot(time_data, stddevs, 'b', label='Std Dev')
xlabel('Year')
ylabel(standard_name)
legend(loc=2, ncol=1, borderaxespad=0.5)
grid(True)
show()

# <codecell>


