# -*- coding: utf-8 -*-
# <nbformat>3.0</nbformat>

# <markdowncell>

# #Bering Seabirds and Future Climate Variability
# Assessing the projected change in climate variability of areas used by seabirds in the Bering Sea.

# <codecell>

#A list of imports we need for code later in the notebook.
#The css_styles() function must go last.
%matplotlib inline
from owslib.wfs import WebFeatureService
import json
from utilities import find_dict_keys
from shapely.geometry import shape, MultiPolygon
from shapely.geometry import box

import folium
from utilities import get_coords
from IPython.core.display import HTML

import time
import numpy as np
from numpy import ma
import netCDF4

import matplotlib as mpl
from matplotlib import cm
import matplotlib.pyplot as plt
from matplotlib import ticker

from utilities import css_styles
css_styles()

# <markdowncell>

# ####Load Important Bird Areas
# These are defined by [Audobon](http://web4.audubon.org/bird/iba/) and hosted by AOOS on a geoserver. Currently, they represent areas important for numerous species of birds, but we have the single-species polygons and are working on ingesting them so they can be accessed in the same way.

# <codecell>

known_wfs = "http://solo.axiomalaska.com/geoserver/audubon/ows"
wfs = WebFeatureService(known_wfs, version='1.0.0')
geojson_response = wfs.getfeature(typename=['audubon:audubon_ibas'], outputFormat="application/json", srsname="urn:x-ogc:def:crs:EPSG:4326").read()
geojson = json.loads(geojson_response)    
geometries = find_dict_keys("geometry", geojson)
shapes = [shape(g) for g in geometries]

# <markdowncell>

# ### Set a Spatial Filter
# For now, this just uses a bounding box for each IBA rather than polygons. In the future we'll use the actual polygons. (It turns out that using polygon masks in the python numpy and panda libraries is not exactly straightfoward. So we will have to build a function to do that.)

# <codecell>

mapper = folium.Map(location=[65.1, -150.6], zoom_start=4)
#Mapy polygons in red
for s in shapes:
    for c in get_coords(s):
        mapper.line(c, line_color='#FF0000', line_weight=2)

#Map bounding boxes of polygons in blue
for s in shapes:
# miny, minx, maxy, maxx
    analysis_box = s.bounds
    bound = box(*analysis_box).boundary.coords
    mapper.line(bound, line_color='#0000FF', line_weight=3)

mapper.lat_lng_popover()
mapper._build_map()
HTML('<iframe srcdoc="{srcdoc}" style="width:100%; height: 535px; border: none"></iframe>'.format(srcdoc=mapper.HTML.replace('"', '&quot;')))

# <markdowncell>

# <div class="error"><strong>Some Polygons Currently Missing</strong> - Comparing the polygons shown below to the [AOOS visualization](http://portal.aoos.org/?v=rand&portal_id=25#module-metadata/8c6e3c7c-4294-11e2-bc24-00219bfe5678/cd96fa58-aeb5-11e2-8a9a-00219bfe5678) shows some differences: (1) Some polygons seem to be missing (e.g., along the Arctic coast), (2) The dot-like polygons along the northern coasts appear to be incomplete polylines. But the bounds for these polygons look correct, so it may just be a strange shapely or folium display issue.</div>

# <markdowncell>

# <div class='success'><strong>shapes.bounds</strong> is the multipolygon array that we want to carry forward.</div>

# <markdowncell>

# ### Load model data
# ####Set Climate basline model from which to pull data
# [PMEL Co-ordinated Ocean-Ice Reference Experiments (CORE) Climate Model](http://portal.aoos.org/?v=rand&portal_id=25#module-metadata/5626a0b6-7d79-11e3-ac17-00219bfe5678/82fde9d7-ccdf-4404-a9bd-c159d9d6461d) - 01/19/1969 - 01/02/2005  
# This is our baseline time-period. In theory, we have two other models which have data over our baseline period, [CFS-R](http://portal.aoos.org/?v=rand&portal_id=25#module-metadata/f8cb79f6-7d59-11e3-a6ee-00219bfe5678/af7b2758-e7d0-4d82-96eb-1696c464586d) and [CFSv2](http://portal.aoos.org/?v=rand&portal_id=25#module-metadata/4e77fed6-7d7a-11e3-97d7-00219bfe5678/0e50760b-8795-4aac-a155-dd624aff9c24), but we didn't recieve the full time-series for either of these models.
# 
# ####Set Climate projections(s) from which to pull data
# [PMEL CCCma](http://portal.aoos.org/alaska-statewide.php#module-metadata/4f706756-7d57-11e3-bce5-00219bfe5678/6b2d1f96-4c5b-4b13-ad57-c8ca2e326a34) - 01/26/2003 - 01/01/2040  
# [PMEL MIROC](http://portal.aoos.org/alaska-statewide.php#module-metadata/68ea728a-7d7a-11e3-823b-00219bfe5678/50242dc9-b639-42c4-8287-d0aeb082fb1f) - 12/29/2002 - 12/04/2039  
# [PMEL ECHO-G](http://portal.aoos.org/alaska-statewide.php#module-metadata/18ffa59c-7d7a-11e3-82a4-00219bfe5678/7bfa82c2-3051-484e-8c7b-9b9f711083f5) - 12/29/2002 - 12/04/2039  
# <div class="warning"><strong>CCCma Model Dates</strong> - CCCma models have slightly different dates. This is because the modeling groups started at slightly different times. Nick Bond says these dates should not be rounded (in order to make them all match up) as it will increase model error.</div>
# 
# Each forecast model produced one time slice per week from 2002 to 2040. Loading these models remotely worked, but they are too large to do analyses over the web. I downloaded these files to my local machine via THREDDS (~294 Gb of files for each model). Unfortunately, by doing the work locally, we lost the advantage of using the ncml virtual aggregated datasets (which makes the whole lot of files looks like a single dataset).Â We also cannot use any of the metadata improvements that AOOS makes using ncml.

# <markdowncell>

# ###Processing Model Results
# The model results are quite large, and we have to think a lot about memory management when trying to process the files and calculate statistics. I tried doing it in python, but calculated that the processing would take over 2 months of CPU time. Whereas it takes a few hours using an external package of binaries called NCO. The software used and specific steps to calculate the mean and standard deviation of the model over time is documented in a [separate notebook](calculate_base_stats.ipynb).

# <markdowncell>

# ###Visualize Base Statistics
# We load in the statistics files produced by NCO in the processing step, and take a look.

# <codecell>

# Global sea surface temperature - just as a test
#model = 'http://thredds.axiomalaska.com/thredds/dodsC/G1_SST.nc'
#standard_name = 'sea_surface_temperature'

original_name='temp_latlon'

#Hindcast
#Get mean value
avg = netCDF4.Dataset('/augie/gluster/data/netCDF/pmel/core/Outputs/core_average.nc')
hind_avg = ma.masked_array(avg.variables[original_name][:])
mask=ma.getmask(core_temp_avg)

#Get Sum of squares by multiplying average square by number of counts
stddev = netCDF4.Dataset('/augie/gluster/data/netCDF/pmel/core/Outputs/core_rmssdn.nc')
hind_stddev = ma.masked_array(stddev.variables[original_name][:], mask)

#Projected
#Get mean value
pavg = netCDF4.Dataset('/augie/gluster/data/netCDF/pmel/cccma/Outputs/cccma_average.nc')
proj_avg = ma.masked_array(pavg.variables[original_name][:])

#Get Sum of squares by multiplying average square by number of counts
pstddev = netCDF4.Dataset('/augie/gluster/data/netCDF/pmel/cccma/Outputs/cccma_rmssdn.nc')
proj_stddev = ma.masked_array(pstddev.variables[original_name][:], mask)

# <codecell>

vmin = 0.0
vmax = 8.0
cmap = 'gist_heat'
origin = 'lower'
interpolation = 'nearest'
orientation = 'horizontal'
shrink = 0.8

def create_subplot(data, pltnum, title):
    plt.subplot(2,2,pltnum)
    plt.title(title)
    img = plt.imshow(data, vmin=vmin, vmax=vmax, 
               origin='lower', interpolation='nearest', cmap = cmap)
    
    return img

#Create subplots
fig = plt.figure(num=None, figsize=(16,10))
cm.gist_heat.set_bad('black', 0.8) # Set masked values to dark gray
img = create_subplot(hind_avg[0,0], 1, 'Core Mean')
img = create_subplot(hind_stddev[0,0], 3, 'Core Std Dev')
img = create_subplot(proj_avg[0,0], 2, 'CCCMA Mean')
img = create_subplot(proj_stddev[0,0], 4, 'CCCMA Std Dev')


#Add pretty colorbar
cax=fig.add_axes([0.25, 0.02, 0.5, 0.03]) #[left, bottom, width, height]
cb = plt.colorbar(img, cax=cax, orientation=orientation, extend='both')
cb.locator = ticker.MaxNLocator(nbins=8)
cb.update_ticks()
plt.text(0.5, -2, 'Degrees C')

plt.show()

# <markdowncell>

# ##Visualize Higher Order Statistics
# Higher order stats will be calculated in [this notebook](calculate_higher_stats.ipynb).  
# We will subset the higher order stats using our polygons in [this notebook](subset_higher_stats.ipynb).

