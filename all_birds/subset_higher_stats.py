# -*- coding: utf-8 -*-
# <nbformat>3.0</nbformat>

# <markdowncell>

# # Subset higher order stats by our bounding boxes

# <codecell>

#A list of imports we need for code later in the notebook.
#The css_styles() function must go last.
%matplotlib inline
from owslib.wfs import WebFeatureService
import json
from utilities import find_dict_keys
from shapely.geometry import shape, MultiPolygon
from shapely.geometry import box

import folium
from utilities import get_coords
from IPython.core.display import HTML

import time
import numpy as np
from numpy import ma
import netCDF4
import pandas as pd
from pandas import Series

import matplotlib as mpl
from matplotlib import cm
import matplotlib.pyplot as plt
from matplotlib import ticker

from utilities import css_styles
css_styles()

# <codecell>

#Load our important bird areas again.
known_wfs = "http://solo.axiomalaska.com/geoserver/audubon/ows"
wfs = WebFeatureService(known_wfs, version='1.0.0')
geojson_response = wfs.getfeature(typename=['audubon:audubon_ibas'], outputFormat="application/json", srsname="urn:x-ogc:def:crs:EPSG:4326").read()
geojson = json.loads(geojson_response)

geometries = find_dict_keys('geometry', geojson)
shapes = [shape(g) for g in geometries]

sitenums = find_dict_keys('sitenum', geojson)
sitenums = [str(s) for s in sitenums]
nsites=len(sitenums)

objectids = find_dict_keys('objectid', geojson)
objectids = [str(s) for s in objectids]

#This generates bounding boxes from the complex geometries in shapes.
minlat = list()
minlon = list()
maxlat = list()
maxlon = list()
for s in shapes:
    minlat.append(s.bounds[0])
    minlon.append(s.bounds[1])
    maxlat.append(s.bounds[2])
    maxlon.append(s.bounds[3])
# miny, minx, maxy, maxx

# <codecell>

#Bring in netCDF datasets
#Set datasets
avg = netCDF4.Dataset('/augie/gluster/data/netCDF/pmel/core/Outputs/core_average.nc')
stddev = netCDF4.Dataset('/augie/gluster/data/netCDF/pmel/core/Outputs/core_rmssdn.nc')
pavg = netCDF4.Dataset('/augie/gluster/data/netCDF/pmel/cccma/Outputs/cccma_average.nc')
pstddev = netCDF4.Dataset('/augie/gluster/data/netCDF/pmel/cccma/Outputs/cccma_rmssdn.nc')

# <codecell>

# Load latitude and longitude arrays
latitude = np.array(avg.variables['LATITUDE'])
longitude = np.array(avg.variables['LONGITUDE'])

# <codecell>

#This is our known metadata for the outputs
outputmeta = pd.DataFrame({
       'orgName': ['icephl_latlon',
                 'phs_latlon','phs_latlon','phs_latlon',
                 'phl_latlon','phl_latlon','phl_latlon',
                 'mzl_latlon','mzl_latlon','mzl_latlon',
                 'cop_latlon','cop_latlon','cop_latlon',
                 'ncao_latlon','ncao_latlon','ncao_latlon',
                 'ncas_latlon','ncas_latlon','ncas_latlon',
                 'eup_latlon','eup_latlon','eup_latlon',
                 'det_latlon','det_latlon','det_latlon',
                 'ben_latlon',
                 'temp_latlon','temp_latlon','temp_latlon',
                 'aice_latlon',
                 'u_latlon','u_latlon','u_latlon',
                 'v_latlon','v_latlon','v_latlon'],
       'name': ['Ice Phytoplankton Concentration',
            'Small Phytoplankton Concentration 0-5m','Small Phytoplankton Concentration 10-60m','Small Phytoplankton Concentration 60-200m',
            'Large Phytoplankton Concentration 0-5m','Large Phytoplankton Concentration 10-60m','Large Phytoplankton Concentration 60-200m',
            'Large Microzooplankton Concentration 0-5m','Large Microzooplankton Concentration 10-60m','Large Microzooplankton Concentration 60-200m',
            'Small Coastal Copepod Concentration 0-5m','Small Coastal Copepod Concentration 10-60m','Small Coastal Copepod Concentration 60-200m',
            'Offshore Neocalanus Concentration 0-5m','Offshore Neocalanus Concentration 10-60m','Offshore Neocalanus Concentration 60-200m',
            'Neocalanus Concentration 0-5m','Neocalanus Concentration 10-60m','Neocalanus Concentration 60-200m',
            'Euphausiids Concentration 0-5m','Euphausiids Concentration 10-60m','Euphausiids Concentration 60-200m',
            'Detritus Concentration 0-5m','Detritus Concentration 10-60m','Detritus Concentration 60-200m',
            'Benthos Concentration',
            'Sea Water Temperature 0-5m','Sea Water Temperature 10-60m','Sea Water Temperature 60-200m',
            'Sea Ice Area Fraction',
            'Zonal (U) Current 0-5m','Zonal (U) Current 10-60m','Zonal (U) Current 60-200m',
            'Meridional (V) Current 0-5m','Meridional (V) Current 10-60m','Meridional (V) Current 60-200m'],
       'units': ['mgC/m2',
             'mgC/m3','mgC/m3','mgC/m3',
             'mgC/m3','mgC/m3','mgC/m3',
             'mgC/m3','mgC/m3','mgC/m3',
             'mgC/m3','mgC/m3','mgC/m3',
             'mgC/m3','mgC/m3','mgC/m3',
             'mgC/m3','mgC/m3','mgC/m3',
             'mgC/m3','mgC/m3','mgC/m3',
             'mgC/m3','mgC/m3','mgC/m3',
             'mgC/m2',
             'degrees C','degrees C','degrees C',
             'Fraction',
             'm/s','m/s','m/s',
             'm/s','m/s','m/s']
       })

# <codecell>

outputmeta

# <markdowncell>

# Some shapes will be empty (i.e., there's a polygon, but no model data), others will have very few values. We should keep track of how many model pixels are going into each resulting mean. For the combined IBAs, there are 210 polygons, identified by sitenums, objectids, and their geometries.

# <markdowncell>

# ##Store the results by keeping track of the data in a Pandas Dataframe
# We will append columns to these and fill in the data as we get it.

# <codecell>

#Create a dataframe to house the site information, indexed by the IBA sitenums.
siteinfo = pd.DataFrame(
                        {
                        'objectid': objectids,
                        'minlat': minlat,
                        'minlon': minlon,
                        'maxlat': maxlat,
                        'maxlon': maxlon,
                        'tested': (['no'] * nsites)
                        },
                        index = sitenums,
                        columns = ['sitenum', 'objectid', 'minlat', 'maxlat', 'minlon', 'maxlon',
                                  'miny', 'maxy', 'minx', 'maxx', 'tested']
                       )

# <markdowncell>

# Now convert the latlons to pixel indices for the model.

# <codecell>

latindices = list()
lonindices = list()

for i in range(0, nsites-1):
    #first do latitude
    indicesy = np.where(np.logical_and(latitude <= maxlat[i],
                                       latitude >= minlat[i]))
    latindices.append(indicesy[0])
    #then longitude
    #PMEL models are in positive east, so we have to convert our bounding box.
    indicesx = np.where(np.logical_and(longitude[:] <= maxlon[i]+360,
                                       longitude[:] >= minlon[i]+360))
    lonindices.append(indicesx[0])

# <codecell>

#finds the minimum and maxium pixel indices for each sitenum
for i in range(0, nsites-1):
    if len(latindices[i]) > 0:
        siteinfo.miny[i] = min(latindices[i])
        siteinfo.maxy[i] = max(latindices[i])
    if len(lonindices[i]) > 0:
        siteinfo.minx[i] = min(lonindices[i])
        siteinfo.maxx[i] = max(lonindices[i]) 
#This tests to see if there are any NaNs, meaning the IBA goes off the edge of the model.
#We'll skip any of those IBAs by setting a "Tested" variable to 0.
    if pd.notnull(siteinfo.miny[i]):
        if pd.notnull(siteinfo.maxy[i]):
            if pd.notnull(siteinfo.minx[i]):
                if pd.notnull(siteinfo.maxx[i]):
                    siteinfo.tested[i] = 'yes'

# <codecell>

siteinfo.tail()

# <markdowncell>

# Now that we have the pixel mins and maxes, we can subset the avg, stddev, pavg, and pstddev arrays, average those values, and add them to the dataframe as well. We also want to implement some sort of depth binning here.

# <codecell>

#Create a dataframe to house the output data, indexed by the IBA sitenums.
outputs = pd.DataFrame(
                        index = sitenums,
                        columns = list(metadata.name)
                       )

# <codecell>

outputs.head()

# <codecell>

for i in range(0, nsites-1):
    if siteinfo.tested[i] == 'yes':
        

# <codecell>

print metadata.name

# <codecell>

outputs2 = pd.DataFrame({'sitenum': sitenums,
                        }, columns = ['sitenum', 'jack'])

# <codecell>

outputs2

# <codecell>

outputs2.head()

# <codecell>


