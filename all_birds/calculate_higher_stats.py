# -*- coding: utf-8 -*-
# <nbformat>3.0</nbformat>

# <markdowncell>

# #Calculate Higher Level Statistics
# This notbook uses [base statistics](calculate_base_stats.ipynb) to calculate higher order stats to assess Seabird Vulnerability in the Bering Sea

# <codecell>

#A list of imports we need for code later in the notebook.
#The css_styles() function must go last.
%matplotlib inline
from owslib.wfs import WebFeatureService
import json
from utilities import find_dict_keys
from shapely.geometry import shape, MultiPolygon
from shapely.geometry import box

import folium
from utilities import get_coords
from IPython.core.display import HTML

import time
import numpy as np
from numpy import ma
import netCDF4

import calendar
from datetime import datetime

import matplotlib as mpl
from matplotlib import cm
import matplotlib.pyplot as plt
from matplotlib import ticker

from utilities import css_styles
css_styles()

# <markdowncell>

# ###Variable Names
# Because we're accessing raw NetCDF files instead of aggregated and curated NcML, we need to access the data using the original variable names.

# <codecell>

#Variables with time and depth coordinates
#biological
variable_index = 10

original_names=['icephl_latlon',
                'phs_latlon',
                'phl_latlon',
                'mzl_latlon',
                'cop_latlon',
                'ncao_latlon',
                'ncas_latlon',
                'eup_latlon',
                'det_latlon',
                'ben_latlon',
                'temp_latlon',
                'aice_latlon',
                'u_latlon',
                'v_latlon'
                ]

standard_names=[
                'ice_phytoplankton_concentration', #(no depth)
                'small_phytoplankton_concentration',
                'large_phytoplankton_concentration',
                'large_microzooplankton_concentration',
                'small_coastal_copepod_concentration',
                'offshore_neocalanus_concentration',
                'neocalanus_concentration',
                'euphausiids_concentration',
                'detritus_concentration',
                'benthos_concentration', #benthic infauna
                'sea_water_temperature',
                'salinity',
                'sea_ice_area_fraction',
                'u_current', #Zonal Current 
                'v_current' #Meridional Current
                ]

# <markdowncell>

# ### Load data

# <codecell>

# Global sea surface temperature - just as a test
#model = 'http://thredds.axiomalaska.com/thredds/dodsC/G1_SST.nc'
#standard_name = 'sea_surface_temperature'

#Hindcast
#Get mean value
nc = netCDF4.Dataset('/augie/gluster/data/netCDF/pmel/core/Outputs/core_average.nc')
core_temp_avg = ma.masked_array(avg.variables['temp_latlon'][:])
mask=ma.getmask(core_temp_avg)

#Get Sum of squares by multiplying average square by number of counts
stddev = netCDF4.Dataset('/augie/gluster/data/netCDF/pmel/core/Outputs/core_rmssdn.nc')
core_temp_stddev = ma.masked_array(stddev.variables['temp_latlon'][:], mask)

#Projected
#Get mean value
pavg = netCDF4.Dataset('/augie/gluster/data/netCDF/pmel/cccma/Outputs/cccma_average.nc')
cccma_temp_avg = ma.masked_array(pavg.variables['temp_latlon'][:])

#Get Sum of squares by multiplying average square by number of counts
pstddev = netCDF4.Dataset('/augie/gluster/data/netCDF/pmel/cccma/Outputs/cccma_rmssdn.nc')
cccma_temp_stddev = ma.masked_array(pstddev.variables['temp_latlon'][:], mask)

# <codecell>

standard_names[variable_index]
variable = nc.variables[original_names[variable_index]]

#from utilities import get_variable_from_standard
#variable = get_variable_from_standard(nc, standard_names[variable_index])[0]

print variable.dimensions
print variable.shape

# <markdowncell>

# ### Set a time filter
# This is the time that we will search over in the model data. We'll eventually use this aggregate 

# <codecell>

#Base Time Frame
base_starttime = '1970-01-01'
base_stoptime = '2000-12-31'

#Projected Time Frame
proj_starttime = '2010-01-01'  # UTC
proj_stoptime = '2010-12-01' # short time frame, just to test the system
#proj_stoptime = "2039-12-01"  # UTC - can't be 2040 because of the CCCma model

import pytz
from dateutil.parser import parse
min_time = parse(base_starttime).replace(tzinfo=pytz.utc)
max_time = parse(base_stoptime).replace(tzinfo=pytz.utc)

# <markdowncell>

# The following cell tries to match up the time filter (in UTC) to whatever the time units are in the model data (in this case listed in seconds).

# <codecell>

time_var = nc.variables['TIME']
time_units = "seconds since 1900-01-01 00:00:00"
#time_var = get_variable_from_standard(nc, 'TIME')[0]

#The first part of this IF statement was because our test model (G1SST) had a bad time conversion.
#It may no longer be necessary (but keep the ELSE).
if time_var.dtype == np.dtype('S1'):
    def str_to_epoch(date_str):
        return calendar.timegm(parse(date_str).replace(tzinfo=pytz.utc).timetuple())
    new_time = netCDF4.chartostring(time_var[:])
    
    #time_var.units = "seconds since 1970-01-01"
    new_min_time = calendar.timegm(min_time.timetuple())
    new_max_time = calendar.timegm(max_time.timetuple())
    np_str_to_epoch = np.vectorize(str_to_epoch)
    new_time = np_str_to_epoch(new_time)
    time_indices = np.logical_and(new_time[:] <= new_max_time,
                                  new_time[:] >= new_min_time)
    time_indices = np.where(time_indices)
    time_data = new_time[time_indices]
    time_data = netCDF4.num2date(time_data, time_var.units)
else:
    new_min_time, new_max_time = netCDF4.date2num([min_time, max_time], time_units)
    time_indices = np.logical_and(time_var[:] <= new_max_time,
                                  time_var[:] >= new_min_time)
    time_indices = np.where(time_indices)
    time_data = time_var[time_indices]
#    time_data = netCDF4.num2date(time_data, time_units)

# <markdowncell>

# ### Set a Depth Filter
# We want a series of bins that go from 0-10; 10-60; and 60-200.

# <codecell>

minz = 0
maxz = 10

depth_var = get_variable_from_standard(nc, "depth")[0]
depth_indices = np.logical_and(depth_var[:] <= maxz,
                             depth_var[:] >= minz)
depth_indices = np.where(depth_indices)
depth_data = depth_var[depth_indices]

print depth_data.size

# <markdowncell>

# ### Subset the Model Variable
# Print our previously found time, depth, latitude and longitude indices. Note: we have to be careful here because variable is a masked array. We only want to add to n the values that are unmasked (i.e., have data). Here I'm using the compressed() method, which loses the spatial component but returns just unmasked values .

# <codecell>

print time_indices[0]
time_indices[0]
print depth_indices[0]
print variable.shape
print variable

# <codecell>

starttime = time.time()
sum_x = np.float64(0.)
sum_x2 = np.float64(0.)
n = np.int64(0)

print 'Start Time: ' + str(min_time)
print 'Stop Time: ' + str(max_time)

for t in time_indices[0]:
    for d in depth_indices[0]: # this is safety, in case the depth bin is very large
        flatslice=variable[t, d,:,:].compressed() # returns a 1D array of non-masked values
        sum_x = sum_x + np.sum(flatslice)
        sum_x2 = sum_x2 + np.sum(np.square(flatslice))
        n = n + flatslice.size
    print time_data[t-time_indices[0][0]]
    
marktime = time.time()-starttime
print 'Operation took ' + str(round(marktime/3600, 2)) + ' hours.'

