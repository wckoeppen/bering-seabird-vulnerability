# -*- coding: utf-8 -*-
# <nbformat>3.0</nbformat>

# <markdowncell>

# #Calculate Higher Level Statistics
# This notbook uses [base statistics](calculate_base_stats.ipynb) to calculate higher order stats to assess Seabird Vulnerability in the Bering Sea

# <codecell>

#A list of imports we need for code later in the notebook.
#The css_styles() function must go last.
%matplotlib inline
from owslib.wfs import WebFeatureService
import json
from utilities import find_dict_keys
from shapely.geometry import shape, MultiPolygon
from shapely.geometry import box

import folium
from utilities import get_coords
from IPython.core.display import HTML

import time
import numpy as np
from numpy import ma
import netCDF4
import pandas as pd

import calendar
from datetime import datetime

import matplotlib as mpl
from matplotlib import cm
import matplotlib.pyplot as plt
from matplotlib import ticker

from utilities import css_styles
css_styles()

# <markdowncell>

# ###Load Variable Library
# Because we're accessing raw NetCDF files instead of aggregated and curated NcML, we need to access the data using the original variable names. These can be found using ncdump, or some other netCDF software. I'm loading these into a panda DataFrame.

# <codecell>

#The original metadata was not very good, and not all fields were filled in (e.g., units),
#so we have to correct these.
metadata = pd.DataFrame({'orgName': ['icephl_latlon','phs_latlon','phl_latlon',
                        'mzl_latlon','cop_latlon',
                        'ncao_latlon','ncas_latlon','eup_latlon',
                        'det_latlon','ben_latlon','temp_latlon',
                        'aice_latlon','u_latlon','v_latlon'],
            'name': ['Ice Phytoplankton Concentration','Small Phytoplankton Concentration', 'Large Phytoplankton Concentration',
                     'Large Microzooplankton Concentration', 'Small Coastal Copepod Concentration',
                     'Offshore Neocalanus Concentration','Neocalanus Concentration', 'Euphausiids Concentration',
                     'Detritus Concentration','Benthos Concentration','Sea Water Temperature',
                     'Sea Ice Area Fraction','Zonal (U) Current','Meridional (V) Current'],
            'units': ['mgC/m2','mgC/m3','mgC/m3',
                      'mgC/m3','mgC/m3',
                      'mgC/m3','mgC/m3','mgC/m3',
                      'mgC/m3','mgC/m2','degrees C',
                      'Fraction','m/s', 'm/s']})
metadata

# <markdowncell>

# ###Load in Variables, Compare Values
# We want to know: compared to our baseline period, which areas are going to have increased variability, which areas will have decreased variability?
# 
# To do this, we'll calculate the average standard deviation of the baseline period, and subtract it from the average standard deviation of the projected period. This is slightly different than Melanie Smith's math, which uses the standard deviation over space rather than over time.

# <codecell>

#Hindcast
#Set datasets
avg = netCDF4.Dataset('/augie/gluster/data/netCDF/pmel/core/Outputs/core_average.nc')
stddev = netCDF4.Dataset('/augie/gluster/data/netCDF/pmel/core/Outputs/core_rmssdn.nc')
pavg = netCDF4.Dataset('/augie/gluster/data/netCDF/pmel/cccma/Outputs/cccma_average.nc')
pstddev = netCDF4.Dataset('/augie/gluster/data/netCDF/pmel/cccma/Outputs/cccma_rmssdn.nc')

#Get a mask from the first variable
sample = ma.masked_array(avg.variables[metadata.orgName[0]][:])
mask=ma.getmask(sample)

# <codecell>

#This cell defines a plotting scheme that we can use to make consistent plots.

cmap = 'bwr'#'gist_heat'
origin = 'lower'
interpolation = 'nearest'
orientation = 'vertical'
shrink = 0.75

def create_subplot(data, pltnum, title, unitlabel):
    bound = np.max(np.absolute([np.min(data), np.max(data)]))
    vmin = -bound
    vmax = bound
    
    plt.subplot(1,2,pltnum)
    plt.title(title)
    img = plt.imshow(data, vmin=vmin, vmax=vmax, 
               origin=origin, interpolation=interpolation, cmap = cmap)
#    cm.gist_heat.set_bad('black', 0.8) # Set masked values to dark gray
    cb = plt.colorbar(img, orientation=orientation, extend='both', shrink=shrink, label=unitlabel)
    cm.bwr.set_bad('black', 0.8) # Set masked values to dark gray
    cb.locator = ticker.MaxNLocator(nbins=8)
    cb.update_ticks()
    return img

# <codecell>

#This loop goes through each variable, pulls out the mean and standard deviation for the
#projected and hindcasts models, and plots the differences between the two time periods.

for i in range(len(metadata['name'])):
    #Get a variable, just to see how many dimensions it has.
    hind_avg = ma.squeeze(ma.masked_array(avg.variables[metadata.orgName[i]][:]))
    
    if len(hind_avg.shape) > 2: # this takes just the surface value
        hind_avg =    ma.squeeze(ma.masked_array(avg.variables[metadata.orgName[i]][:])[0,0])
        hind_stddev = ma.squeeze(ma.masked_array(stddev.variables[metadata.orgName[i]][:])[0,0])
        proj_avg =    ma.squeeze(ma.masked_array(pavg.variables[metadata.orgName[i]][:])[0,0])
        proj_stddev = ma.squeeze(ma.masked_array(pstddev.variables[metadata.orgName[i]][:])[0,0])
    else:
        hind_stddev = ma.squeeze(ma.masked_array(stddev.variables[metadata.orgName[i]][:])[0])
        proj_avg =    ma.squeeze(ma.masked_array(pavg.variables[metadata.orgName[i]][:])[0])
        proj_stddev = ma.squeeze(ma.masked_array(pstddev.variables[metadata.orgName[i]][:])[0])
    #Create subplots
    fig = plt.figure(figsize=(16,5))
    plt.suptitle(metadata.name[i], fontsize=20)
    create_subplot(proj_avg-hind_avg, 1, 'Projected Mean - Historical Mean', metadata.units[i])
    create_subplot(proj_stddev-hind_stddev, 2, 'Projected StdDev - Historical StdDev', metadata.units[i])
    # This line saves the next plots to files
    #    fig.savefig('/home/will/Desktop/' + metadata.orgName[i],dpi=200)

    plt.show()

# <codecell>

vmin = -5
vmax = 5.0
cmap = 'seismic'#'gist_heat'
origin = 'lower'
interpolation = 'nearest'
orientation = 'vertical'
shrink = 0.8

def create_subplot(data, pltnum, title):
    bound = np.max(np.absolute([np.min(data), np.max(data)]))
    vmin = -bound
    vmax = bound
    
    plt.subplot(1,2,pltnum)
    plt.title(title)
    img = plt.imshow(data, vmin=vmin, vmax=vmax, 
               origin=origin, interpolation=interpolation, cmap = cmap)
#    cm.gist_heat.set_bad('black', 0.8) # Set masked values to dark gray
    cb = plt.colorbar(img, orientation=orientation, extend='both', shrink=0.75, label='degrees C')
    cm.seismic.set_bad('black', 0.8) # Set masked values to dark gray
    cb.locator = ticker.MaxNLocator(nbins=8)
    cb.update_ticks()
    return img

#Create subplots
fig = plt.figure(figsize=(16,5))

create_subplot(proj_avg[0,0]-hind_avg[0,0], 1, 'Projected Mean - Historical Mean')
create_subplot(proj_stddev[0,0]-hind_stddev[0,0], 2, 'Projected StdDev - Historical StdDev')
plt.suptitle('Temperature', fontsize=20)
#cm.gist_heat.set_bad('black', 0.8) # Set masked values to dark gray

plt.show()

# <markdowncell>

# ### Set a time filter
# This is the time that we will search over in the model data. We'll eventually use this aggregate 

# <codecell>

#Base Time Frame
base_starttime = '1970-01-01'
base_stoptime = '2000-12-31'

#Projected Time Frame
proj_starttime = '2010-01-01'  # UTC
proj_stoptime = '2010-12-01' # short time frame, just to test the system
#proj_stoptime = "2039-12-01"  # UTC - can't be 2040 because of the CCCma model

import pytz
from dateutil.parser import parse
min_time = parse(base_starttime).replace(tzinfo=pytz.utc)
max_time = parse(base_stoptime).replace(tzinfo=pytz.utc)

# <markdowncell>

# The following cell tries to match up the time filter (in UTC) to whatever the time units are in the model data (in this case listed in seconds).

# <codecell>

time_var = nc.variables['TIME']
time_units = "seconds since 1900-01-01 00:00:00"
#time_var = get_variable_from_standard(nc, 'TIME')[0]

#The first part of this IF statement was because our test model (G1SST) had a bad time conversion.
#It may no longer be necessary (but keep the ELSE).
if time_var.dtype == np.dtype('S1'):
    def str_to_epoch(date_str):
        return calendar.timegm(parse(date_str).replace(tzinfo=pytz.utc).timetuple())
    new_time = netCDF4.chartostring(time_var[:])
    
    #time_var.units = "seconds since 1970-01-01"
    new_min_time = calendar.timegm(min_time.timetuple())
    new_max_time = calendar.timegm(max_time.timetuple())
    np_str_to_epoch = np.vectorize(str_to_epoch)
    new_time = np_str_to_epoch(new_time)
    time_indices = np.logical_and(new_time[:] <= new_max_time,
                                  new_time[:] >= new_min_time)
    time_indices = np.where(time_indices)
    time_data = new_time[time_indices]
    time_data = netCDF4.num2date(time_data, time_var.units)
else:
    new_min_time, new_max_time = netCDF4.date2num([min_time, max_time], time_units)
    time_indices = np.logical_and(time_var[:] <= new_max_time,
                                  time_var[:] >= new_min_time)
    time_indices = np.where(time_indices)
    time_data = time_var[time_indices]
#    time_data = netCDF4.num2date(time_data, time_units)

# <markdowncell>

# ### Set a Depth Filter
# We want a series of bins that go from 0-10; 10-60; and 60-200.

# <codecell>

minz = 0
maxz = 10

depth_var = get_variable_from_standard(nc, "depth")[0]
depth_indices = np.logical_and(depth_var[:] <= maxz,
                             depth_var[:] >= minz)
depth_indices = np.where(depth_indices)
depth_data = depth_var[depth_indices]

print depth_data.size

# <markdowncell>

# ### Subset the Model Variable
# Print our previously found time, depth, latitude and longitude indices. Note: we have to be careful here because variable is a masked array. We only want to add to n the values that are unmasked (i.e., have data). Here I'm using the compressed() method, which loses the spatial component but returns just unmasked values .

# <codecell>

print time_indices[0]
time_indices[0]
print depth_indices[0]
print variable.shape
print variable

# <codecell>

starttime = time.time()
sum_x = np.float64(0.)
sum_x2 = np.float64(0.)
n = np.int64(0)

print 'Start Time: ' + str(min_time)
print 'Stop Time: ' + str(max_time)

for t in time_indices[0]:
    for d in depth_indices[0]: # this is safety, in case the depth bin is very large
        flatslice=variable[t, d,:,:].compressed() # returns a 1D array of non-masked values
        sum_x = sum_x + np.sum(flatslice)
        sum_x2 = sum_x2 + np.sum(np.square(flatslice))
        n = n + flatslice.size
    print time_data[t-time_indices[0][0]]
    
marktime = time.time()-starttime
print 'Operation took ' + str(round(marktime/3600, 2)) + ' hours.'

